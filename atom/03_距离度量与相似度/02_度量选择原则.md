# 度量选择原则

> 学习目标：掌握向量数据库中三种主要度量（内积、余弦相似度、欧氏距离）的选择策略，能根据实际场景做出正确选择

---

## 1. 【30字核心】

**度量选择决定"相似"的定义：内积看方向+长度，余弦只看方向，欧氏距离看空间距离，需根据数据特性和业务需求选择。**

---

---

## 2. 【第一性原理】

### 什么是第一性原理？

**第一性原理**：回到事物最基本的真理，从源头思考问题

### 度量选择的第一性原理 🎯

#### 1. 最基础的问题

**核心问题：如何定义两个向量的"相似"？**

这个问题没有标准答案，取决于：
- 向量表示什么（语义？位置？特征？）
- 什么对业务重要（方向？长度？距离？）
- 计算资源限制（速度？内存？）

**三种度量的回答：**

```
内积(IP)：相似 = 方向一致 × 长度相当
         数学：a·b = Σ(aᵢ×bᵢ)

余弦(Cosine)：相似 = 方向一致
             数学：cos(θ) = (a·b)/(‖a‖×‖b‖)

欧氏(L2)：相似 = 空间距离近
         数学：d = √Σ(aᵢ-bᵢ)²
```

---

#### 2. 为什么需要不同的度量？

**根本原因：不同场景对"相似"的定义不同**

**场景分析：**

```
场景1：语义搜索
  - 问题：找语义相近的文本
  - "相似" = 语义方向一致
  - 向量长度无意义（都是embedding）
  - 选择：Cosine 或 归一化IP

场景2：推荐系统
  - 问题：找相关且热门的内容
  - "相似" = 相关性 × 热门度
  - 向量长度 = 热门度/权重
  - 选择：IP（保留长度）

场景3：图像聚类
  - 问题：把相似图像分到一组
  - "相似" = 特征空间距离近
  - 需要真实距离做聚类算法
  - 选择：L2（欧氏距离）

场景4：异常检测
  - 问题：找离群点
  - "异常" = 距离正常点远
  - 需要距离概念
  - 选择：L2（欧氏距离）
```

---

#### 3. 度量选择的三层价值

##### 价值1：业务层面 - 定义目标

**度量 = 对业务目标的数学表达**

```python
# 不同业务目标的度量选择

def search_strategy(business_goal):
    """根据业务目标选择度量"""
    strategies = {
        "语义相关": "Cosine/IP",      # 只看内容
        "语义+热门": "IP",             # 内容×权重
        "空间聚类": "L2",              # 真实距离
        "异常检测": "L2",              # 离群距离
    }
    return strategies.get(business_goal, "Cosine")
```

##### 价值2：数学层面 - 保证正确性

**不同度量有不同的数学性质**

```
内积的性质：
- 对称：IP(a,b) = IP(b,a)
- 线性：IP(ca,b) = c×IP(a,b)
- 不是距离（可以为负）

欧氏距离的性质：
- 非负：L2(a,b) ≥ 0
- 对称：L2(a,b) = L2(b,a)
- 三角不等式：L2(a,c) ≤ L2(a,b) + L2(b,c)
- 是真正的"度量/距离"

余弦相似度：
- 值域：[-1, 1]
- 不受长度影响
- 不满足三角不等式（不是距离）
```

**这些性质影响算法选择**：
- 聚类算法需要真实距离（L2）
- 某些索引算法需要三角不等式

##### 价值3：工程层面 - 优化性能

**不同度量的计算复杂度不同**

```python
# 计算复杂度分析

# 内积：O(n) 乘法 + O(n) 加法
def inner_product(a, b):
    return sum(a[i] * b[i] for i in range(len(a)))

# 余弦：内积 + 2个范数 + 1除法
def cosine(a, b):
    ip = inner_product(a, b)
    norm_a = sqrt(sum(x**2 for x in a))
    norm_b = sqrt(sum(x**2 for x in b))
    return ip / (norm_a * norm_b)

# L2：O(n) 减法 + O(n) 平方 + 1开根号
def euclidean(a, b):
    return sqrt(sum((a[i] - b[i])**2 for i in range(len(a))))

# 复杂度排序：IP < Cosine < L2
```

**工程优化策略**：
- 预计算范数，Cosine降为IP复杂度
- 预归一化向量，IP=Cosine
- 批量计算用矩阵乘法

---

#### 4. 从第一性原理推导最佳实践

**问题：** 设计一个高效的向量数据库搜索系统

**从第一性原理思考：**

```
步骤1：分析需求
  - 大多数场景是语义搜索
  - "相似" = 语义方向一致
  - 向量长度通常无意义
  ↓
步骤2：选择度量
  - 语义搜索 → Cosine或IP
  - Cosine计算有额外开销
  ↓
步骤3：优化策略
  - 预归一化所有向量
  - 用IP代替Cosine（结果相同，更快）
  ↓
步骤4：工程实现
  - 存储归一化向量
  - 索引用IP度量
  - 批量计算用矩阵乘法
  ↓
结论：归一化 + IP = 最佳实践
```

---

#### 5. 特殊情况的处理

**场景1：需要保留长度信息**
```python
# 不归一化，直接用IP
# 长度信息作为"权重/热门度"
similarity = np.dot(query, doc)  # 包含长度影响
```

**场景2：需要真实距离**
```python
# 用L2，但注意高维问题
distance = np.linalg.norm(query - doc)
# 考虑降维或用近似方法
```

**场景3：混合需求**
```python
# 两阶段：先语义过滤，再按热门排序
semantic_scores = cosine_similarity(query, docs)  # 语义
popularity_scores = doc_lengths  # 热门度
final_scores = 0.7 * semantic_scores + 0.3 * popularity_scores
```

---

#### 6. 一句话总结第一性原理

**度量选择的本质是定义"相似"：内积看全面（方向+长度），余弦看方向，欧氏看距离；大多数语义搜索场景，"归一化+IP"是最佳选择。**

---

---

## 3. 【3个核心概念】

### 核心概念1：度量定义了"相似"的含义 🎯

**一句话：** 选择度量 = 定义业务上什么叫"相似"

**详细解释：**

三种度量对"相似"的定义完全不同：

```python
import numpy as np

# 三个向量
query = np.array([1, 0])
doc_A = np.array([2, 0])      # 方向同，距离远
doc_B = np.array([0.9, 0.4])  # 方向略偏，距离近

# 不同度量的判断

# 内积：方向+长度
ip_A = np.dot(query, doc_A)  # 2
ip_B = np.dot(query, doc_B)  # 0.9
print(f"IP: A({ip_A}) > B({ip_B}) → A更相似")

# 余弦：只看方向
cos_A = np.dot(query, doc_A) / (np.linalg.norm(query) * np.linalg.norm(doc_A))  # 1.0
cos_B = np.dot(query, doc_B) / (np.linalg.norm(query) * np.linalg.norm(doc_B))  # 0.91
print(f"Cosine: A({cos_A:.2f}) > B({cos_B:.2f}) → A更相似")

# 欧氏距离：空间距离
l2_A = np.linalg.norm(query - doc_A)  # 1.0
l2_B = np.linalg.norm(query - doc_B)  # 0.41
print(f"L2: B({l2_B:.2f}) < A({l2_A:.2f}) → B更相似")
```

**业务映射：**

| 业务需求 | 选择度量 | "相似"的含义 |
|---------|---------|-------------|
| 语义搜索 | Cosine/IP | 语义方向一致 |
| 热门推荐 | IP | 语义相关且热门 |
| 图像聚类 | L2 | 空间上接近 |
| 异常检测 | L2 | 距离正常点远 |

**记忆：** 度量不只是技术选择，而是业务决策！

---

### 核心概念2：归一化是统一度量的桥梁 🌉

**一句话：** 归一化后，IP = Cosine，且与L2有固定关系

**数学关系：**

```
对归一化向量（‖a‖ = ‖b‖ = 1）：

1. IP = Cosine
   IP(a,b) = a·b = Cosine(a,b)

2. L2² = 2(1 - Cosine)
   ‖a-b‖² = 2(1 - a·b)

3. 三者可互相转换
   Cosine = IP = 1 - L2²/2
```

**代码验证：**
```python
import numpy as np

def normalize(v):
    return v / np.linalg.norm(v)

# 归一化向量
a = normalize(np.array([3, 4, 5]))
b = normalize(np.array([1, 2, 3]))

# 计算三种度量
ip = np.dot(a, b)
cos = np.dot(a, b) / (np.linalg.norm(a) * np.linalg.norm(b))
l2 = np.linalg.norm(a - b)

print(f"IP: {ip:.6f}")
print(f"Cosine: {cos:.6f}")
print(f"L2: {l2:.6f}")

# 验证关系
print(f"\n验证 IP = Cosine: {np.isclose(ip, cos)}")
print(f"验证 L2² = 2(1-Cos): {np.isclose(l2**2, 2*(1-cos))}")
```

**实际意义：**
- 归一化后，三种度量的排序结果一致（L2顺序相反）
- 可以用最快的IP计算，得到等价于Cosine的结果
- 这就是为什么向量数据库推荐"归一化+IP"

---

### 核心概念3：维度灾难影响度量有效性 🌀

**一句话：** 高维空间中，欧氏距离的区分度下降

**维度灾难现象：**
```python
import numpy as np

def test_high_dim(dim):
    """测试高维空间中距离的分布"""
    np.random.seed(42)
    n_points = 1000
    
    # 随机生成高维点
    points = np.random.randn(n_points, dim)
    
    # 计算所有点对之间的L2距离
    from scipy.spatial.distance import pdist
    distances = pdist(points, metric='euclidean')
    
    # 距离的变异系数（标准差/均值）
    cv = np.std(distances) / np.mean(distances)
    
    return cv

# 不同维度的测试
dims = [2, 10, 50, 100, 500, 1000]
print("维度 vs 距离变异系数：")
print("-" * 30)
for dim in dims:
    cv = test_high_dim(dim)
    print(f"维度={dim:<4} 变异系数={cv:.4f}")
```

**输出示例：**
```
维度 vs 距离变异系数：
------------------------------
维度=2    变异系数=0.4234
维度=10   变异系数=0.1923
维度=50   变异系数=0.0891
维度=100  变异系数=0.0634
维度=500  变异系数=0.0284
维度=1000 变异系数=0.0201
```

**分析：**
- 维度越高，距离变异系数越小
- 这意味着所有点之间的距离趋于相等
- L2距离的区分能力下降

**解决方案：**
1. **降维**：PCA、t-SNE等
2. **用余弦/IP**：对方向的区分更稳定
3. **量化**：PQ等方法

**在向量数据库中：**
- embedding通常是768/1536维（很高）
- 余弦/IP比L2更适合高维语义搜索
- 这也是为什么主流向量数据库默认用IP/Cosine

---

---

## 4. 【最小可用】掌握20%解决80%问题

### 3.1 三种主要度量速查表

| 度量 | 公式 | 值域 | 越X越相似 | 是否受长度影响 |
|-----|------|------|---------|--------------|
| **内积(IP)** | Σ(aᵢ×bᵢ) | (-∞,+∞) | 越大 | ✅ 是 |
| **余弦相似度** | IP/(‖a‖×‖b‖) | [-1,1] | 越大 | ❌ 否 |
| **欧氏距离(L2)** | √Σ(aᵢ-bᵢ)² | [0,+∞) | 越小 | ✅ 是 |

### 3.2 快速选择流程图

```
开始
  │
  ▼
embedding是否已归一化？
  │
  ├─ 是 ──▶ 用内积(IP) ──▶ 计算最快，结果=余弦
  │
  └─ 否 ──▶ 向量长度是否有意义？
              │
              ├─ 是（如热门度）──▶ 用内积(IP)
              │
              └─ 否 ──▶ 需要真实空间距离吗？
                          │
                          ├─ 是 ──▶ 用欧氏距离(L2)
                          │
                          └─ 否 ──▶ 用余弦相似度
```

### 3.3 常见场景推荐

| 场景 | 推荐度量 | 原因 |
|------|---------|------|
| 语义搜索（归一化embedding）| IP | 最快，等价于余弦 |
| 语义搜索（未归一化）| Cosine | 不受长度影响 |
| RAG检索 | IP（预归一化）| 主流embedding模型输出归一化向量 |
| 图像相似搜索 | Cosine/IP | 方向比长度更重要 |
| 推荐系统 | IP | 可能需要保留热门度信息 |
| 聚类分析 | L2 | 需要真实空间距离 |
| 异常检测 | L2 | 检测"距离异常远"的点 |

### 3.4 代码：三种度量的计算

```python
import numpy as np

def inner_product(a, b):
    """内积（越大越相似）"""
    return np.dot(a, b)

def cosine_similarity(a, b):
    """余弦相似度（越大越相似，范围[-1,1]）"""
    return np.dot(a, b) / (np.linalg.norm(a) * np.linalg.norm(b))

def euclidean_distance(a, b):
    """欧氏距离（越小越相似）"""
    return np.linalg.norm(a - b)

# 示例
v1 = np.array([1, 2, 3])
v2 = np.array([4, 5, 6])

print(f"内积: {inner_product(v1, v2)}")           # 32
print(f"余弦相似度: {cosine_similarity(v1, v2):.4f}")  # 0.9746
print(f"欧氏距离: {euclidean_distance(v1, v2):.4f}")   # 5.196
```

### 3.5 向量数据库配置示例

```python
# ===== Milvus =====
from pymilvus import Collection

# 创建索引时指定度量
collection.create_index(
    field_name="embedding",
    index_params={
        "metric_type": "IP",      # 或 "COSINE" 或 "L2"
        "index_type": "IVF_FLAT",
        "params": {"nlist": 1024}
    }
)

# ===== Pinecone =====
import pinecone

pinecone.create_index(
    name="my-index",
    dimension=768,
    metric="dotproduct"  # 或 "cosine" 或 "euclidean"
)

# ===== Qdrant =====
from qdrant_client.models import Distance, VectorParams

client.create_collection(
    collection_name="my_collection",
    vectors_config=VectorParams(
        size=768,
        distance=Distance.DOT  # 或 COSINE 或 EUCLID
    )
)
```

**这些知识足以：**
- 理解三种度量的区别
- 根据场景选择正确的度量
- 在主流向量数据库中配置度量

---

---

## 5. 【1个类比】用前端开发理解

### 类比1：CSS选择器的精确度 🎯

**三种度量 = 三种CSS选择器策略**

```css
/* 内积(IP) = ID选择器 + 类选择器 */
/* 既看ID（方向）也看类（长度）*/
#header.important { }

/* 余弦(Cosine) = 只看ID选择器 */
/* 只看ID（方向），忽略类（长度）*/
#header { }

/* 欧氏距离(L2) = 计算选择器距离 */
/* 看DOM树中的节点距离 */
/* 距离越近，优先级越高 */
```

**对应关系：**
| CSS概念 | 度量概念 |
|--------|---------|
| ID（唯一标识）| 向量方向 |
| 类（可重复）| 向量长度 |
| DOM距离 | 空间距离 |

---

### 类比2：React组件匹配策略 📦

```javascript
// 场景：从组件库中找最匹配的组件

// 组件特征向量
const componentFeatures = {
  Button: { interactive: 0.9, visual: 0.7, size: 0.5 },
  Link: { interactive: 0.8, visual: 0.3, size: 0.2 },
  Card: { interactive: 0.2, visual: 0.9, size: 0.8 },
};

// 需求特征
const requirement = { interactive: 0.85, visual: 0.6, size: 0.4 };

// 内积：考虑特征权重的总匹配度
function innerProduct(req, comp) {
  return Object.keys(req).reduce((sum, key) =>
    sum + req[key] * comp[key], 0
  );
}

// 余弦：只看特征方向的匹配度
function cosine(req, comp) {
  const dot = innerProduct(req, comp);
  const normReq = Math.sqrt(Object.values(req).reduce((s, v) => s + v*v, 0));
  const normComp = Math.sqrt(Object.values(comp).reduce((s, v) => s + v*v, 0));
  return dot / (normReq * normComp);
}

// 欧氏距离：特征空间的直接距离
function euclidean(req, comp) {
  return Math.sqrt(
    Object.keys(req).reduce((sum, key) =>
      sum + Math.pow(req[key] - comp[key], 2), 0
    )
  );
}

// 不同策略的选择
console.log("内积策略（考虑重要性）：");
Object.entries(componentFeatures).forEach(([name, features]) => {
  console.log(`  ${name}: ${innerProduct(requirement, features).toFixed(3)}`);
});
// Button最匹配（特征值都比较大）

console.log("\n余弦策略（只看比例）：");
Object.entries(componentFeatures).forEach(([name, features]) => {
  console.log(`  ${name}: ${cosine(requirement, features).toFixed(3)}`);
});
// Button最匹配（方向最接近）

console.log("\n欧氏策略（看绝对差异）：");
Object.entries(componentFeatures).forEach(([name, features]) => {
  console.log(`  ${name}: ${euclidean(requirement, features).toFixed(3)}`);
});
// Button最匹配（距离最近）
```

---

### 类比3：路由匹配策略 🛣️

```javascript
// URL路由匹配的三种策略

const routes = [
  { path: '/users/:id/posts', priority: 10 },      // 具体路由
  { path: '/users/:id', priority: 5 },             // 中间路由
  { path: '/users', priority: 1 },                 // 根路由
];

const request = '/users/123/posts';

// 内积式匹配：路径相似度 × 优先级
// 既看匹配程度，也看路由权重

// 余弦式匹配：只看路径相似度
// 忽略优先级，只看URL模式匹配

// 欧氏式匹配：看路径的编辑距离
// 直接计算字符串差异

// 实际路由器通常用"内积式"：
// 既要路径匹配，也考虑优先级/权重
```

---

### 类比4：状态管理的diff策略 📊

```javascript
// Redux/Vuex状态比较

const prevState = { count: 10, items: 5, loading: false };
const nextState = { count: 12, items: 5, loading: true };

// 内积式比较：加权差异
// 重要字段（count）的变化权重更大
function weightedDiff(prev, next, weights) {
  return Object.keys(prev).reduce((sum, key) =>
    sum + Math.abs(prev[key] - next[key]) * weights[key], 0
  );
}

// 余弦式比较：只看变化方向
// 忽略变化幅度，只看哪些字段在变

// 欧氏式比较：直接计算差异
// 每个字段的变化都等权重
function euclideanDiff(prev, next) {
  return Math.sqrt(
    Object.keys(prev).reduce((sum, key) =>
      sum + Math.pow(prev[key] - next[key], 2), 0
    )
  );
}

// React的shouldComponentUpdate通常用"欧氏式"
// 但可以自定义用"内积式"做智能diff
```

---

### 类比5：API版本兼容性检查 🔌

```javascript
// 检查客户端与API版本的兼容性

const apiVersion = { major: 2, minor: 5, patch: 3 };
const clientVersion = { major: 2, minor: 4, patch: 8 };

// 内积式：加权兼容性分数
// major权重最大，patch权重最小
function weightedCompatibility(api, client) {
  const weights = { major: 100, minor: 10, patch: 1 };
  return Object.keys(api).reduce((score, key) => {
    const diff = Math.abs(api[key] - client[key]);
    return score - diff * weights[key];
  }, 1000);  // 从满分开始扣
}

// 余弦式：版本方向一致性
// 只看版本号比例是否一致

// 欧氏式：版本距离
// 直接计算版本号的空间距离

console.log(`兼容性分数: ${weightedCompatibility(apiVersion, clientVersion)}`);
// major相同(0扣分) + minor差1(扣10分) + patch差5(扣5分) = 985分
```

---

### 类比总结表 📊

| 前端概念 | 内积(IP) | 余弦(Cosine) | 欧氏(L2) |
|---------|---------|-------------|----------|
| CSS选择器 | ID+类组合 | 只看ID | DOM距离 |
| 组件匹配 | 特征加权 | 只看比例 | 特征距离 |
| 路由匹配 | 路径+优先级 | 只看路径 | 编辑距离 |
| 状态diff | 加权变化 | 变化方向 | 直接差异 |
| API版本 | 加权兼容 | 比例一致 | 版本距离 |

---

---

## 6. 【反直觉点】最容易错的3个误区

### 误区1：余弦相似度总是最好的选择 ❌

**为什么错？**
- 余弦只关心方向，完全忽略向量长度
- 某些场景下，长度本身携带重要信息（如文档重要性、热门度）
- 如果embedding已归一化，用内积更快且结果相同

**为什么人们容易这样错？**
- 教程常说"余弦不受长度影响"，听起来像优点
- 忽略了"长度有意义"的场景
- 不知道归一化后内积=余弦

**正确理解：**
```python
import numpy as np

# 场景：文档重要性由向量长度表示
# 长度大 = 更权威/更热门

doc_popular = np.array([0.8, 0.6]) * 10    # 热门文档，长度大
doc_niche = np.array([0.85, 0.55]) * 1     # 冷门文档，方向更接近
query = np.array([1, 0])

# 余弦相似度（只看方向）
def cosine(a, b):
    return np.dot(a, b) / (np.linalg.norm(a) * np.linalg.norm(b))

cos_popular = cosine(query, doc_popular)  # 0.8
cos_niche = cosine(query, doc_niche)      # 0.84

print("余弦相似度：")
print(f"  热门文档: {cos_popular:.4f}")
print(f"  冷门文档: {cos_niche:.4f}")
print(f"  结论: 推荐冷门文档（方向更近）")

# 内积（考虑长度）
ip_popular = np.dot(query, doc_popular)  # 8.0
ip_niche = np.dot(query, doc_niche)      # 0.85

print("\n内积：")
print(f"  热门文档: {ip_popular:.4f}")
print(f"  冷门文档: {ip_niche:.4f}")
print(f"  结论: 推荐热门文档（长度更大）")

# 选择取决于业务需求：
# - 只看语义相似 → 余弦
# - 考虑热门度 → 内积
```

**关键点：** 选择度量 = 定义"相似"的含义

---

### 误区2：欧氏距离和余弦相似度差不多 ❌

**为什么错？**
- 它们度量的是完全不同的东西：
  - **欧氏距离**：空间中两点的直线距离
  - **余弦相似度**：两向量的夹角余弦
- 归一化后有数学联系，但语义不同
- 在高维空间中，欧氏距离的有效性下降（维度灾难）

**为什么人们容易这样错？**
- 两者都可以衡量"相似性"
- 在某些场景下结果相近
- 不了解高维空间的特殊性

**正确理解：**
```python
import numpy as np

# 三个向量
v1 = np.array([1, 0])
v2 = np.array([2, 0])      # 与v1同向，但距离远
v3 = np.array([0.9, 0.4])  # 与v1方向不同，但距离近

def cosine(a, b):
    return np.dot(a, b) / (np.linalg.norm(a) * np.linalg.norm(b))

def euclidean(a, b):
    return np.linalg.norm(a - b)

print(f"与v1=[1,0]比较：")
print(f"{'向量':<15} {'余弦相似度':<12} {'欧氏距离':<12}")
print("-" * 40)
print(f"v2=[2,0]{'':<7} {cosine(v1, v2):<12.4f} {euclidean(v1, v2):<12.4f}")
print(f"v3=[0.9,0.4]{'':<3} {cosine(v1, v3):<12.4f} {euclidean(v1, v3):<12.4f}")

print("\n分析：")
print("  余弦: v2更相似（方向完全相同）")
print("  欧氏: v3更相似（空间距离更近）")
print("  两种度量给出相反的结论！")
```

**输出：**
```
与v1=[1,0]比较：
向量             余弦相似度      欧氏距离
----------------------------------------
v2=[2,0]        1.0000       1.0000
v3=[0.9,0.4]    0.9138       0.4123

分析：
  余弦: v2更相似（方向完全相同）
  欧氏: v3更相似（空间距离更近）
  两种度量给出相反的结论！
```

---

### 误区3：度量选择对结果影响不大 ❌

**为什么错？**
- 度量定义了什么叫"相似"
- 不同度量可能导致完全不同的排序
- 错误的度量会返回不相关的结果

**为什么人们容易这样错？**
- 在某些数据集上，不同度量结果相近
- 使用归一化embedding时，内积≈余弦
- 没有做过对比实验

**正确理解：**
```python
import numpy as np

# 模拟embedding（未归一化）
np.random.seed(42)
query = np.array([0.9, 0.1, 0.3, 0.2])
docs = {
    "文档A": np.array([0.85, 0.15, 0.35, 0.25]) * 0.5,   # 方向近，长度小
    "文档B": np.array([0.7, 0.3, 0.4, 0.3]) * 2.0,      # 方向较远，长度大
    "文档C": np.array([0.88, 0.12, 0.32, 0.22]),        # 方向近，长度适中
    "文档D": np.array([0.1, 0.9, 0.2, 0.1]) * 1.5,      # 方向远，长度大
}

def cosine(a, b):
    return np.dot(a, b) / (np.linalg.norm(a) * np.linalg.norm(b))

def euclidean(a, b):
    return np.linalg.norm(a - b)

def inner_product(a, b):
    return np.dot(a, b)

# 三种度量的排名
print("三种度量对同一查询的排名：\n")

# 内积（越大越好）
ip_scores = {k: inner_product(query, v) for k, v in docs.items()}
ip_rank = sorted(ip_scores.items(), key=lambda x: x[1], reverse=True)
print("内积（IP）排名：")
for i, (name, score) in enumerate(ip_rank, 1):
    print(f"  {i}. {name}: {score:.4f}")

# 余弦（越大越好）
cos_scores = {k: cosine(query, v) for k, v in docs.items()}
cos_rank = sorted(cos_scores.items(), key=lambda x: x[1], reverse=True)
print("\n余弦相似度排名：")
for i, (name, score) in enumerate(cos_rank, 1):
    print(f"  {i}. {name}: {score:.4f}")

# 欧氏距离（越小越好）
euc_scores = {k: euclidean(query, v) for k, v in docs.items()}
euc_rank = sorted(euc_scores.items(), key=lambda x: x[1])
print("\n欧氏距离（L2）排名：")
for i, (name, score) in enumerate(euc_rank, 1):
    print(f"  {i}. {name}: {score:.4f}")

print("\n结论：三种度量的Top-1结果可能完全不同！")
```

**关键点：** 度量选择 = 定义业务目标

---

---

## 7. 【实战代码】一个能跑的例子

```python
import numpy as np

# ===== 1. 三种度量的实现 =====
print("=== 三种度量的实现 ===")

def normalize(v):
    """归一化向量"""
    return v / np.linalg.norm(v)

def inner_product(a, b):
    """内积"""
    return np.dot(a, b)

def cosine_similarity(a, b):
    """余弦相似度"""
    return np.dot(a, b) / (np.linalg.norm(a) * np.linalg.norm(b))

def euclidean_distance(a, b):
    """欧氏距离"""
    return np.linalg.norm(a - b)

v1 = np.array([3, 4])
v2 = np.array([6, 8])

print(f"v1 = {v1}")
print(f"v2 = {v2}")
print(f"内积: {inner_product(v1, v2)}")
print(f"余弦: {cosine_similarity(v1, v2):.4f}")
print(f"欧氏: {euclidean_distance(v1, v2):.4f}")

# ===== 2. 度量选择的影响 =====
print("\n=== 度量选择的影响 ===")

# 查询向量
query = np.array([1, 0])

# 候选文档（未归一化）
docs = {
    "Doc_A": np.array([0.95, 0.1]),     # 方向近，长度~1
    "Doc_B": np.array([10, 0]),         # 方向同，长度大
    "Doc_C": np.array([0.7, 0.7]),      # 方向45°，长度~1
    "Doc_D": np.array([0.5, -0.5]),     # 方向315°，长度~0.7
}

print(f"查询: {query}\n")

# 计算三种度量
results = {}
for name, vec in docs.items():
    results[name] = {
        "IP": inner_product(query, vec),
        "Cosine": cosine_similarity(query, vec),
        "L2": euclidean_distance(query, vec)
    }

# 显示结果
print(f"{'文档':<10} {'IP':<10} {'Cosine':<10} {'L2':<10}")
print("-" * 40)
for name, scores in results.items():
    print(f"{name:<10} {scores['IP']:<10.4f} {scores['Cosine']:<10.4f} {scores['L2']:<10.4f}")

# 各度量的排名
print("\n各度量的排名（最相似排第一）：")

ip_rank = sorted(results.items(), key=lambda x: x[1]["IP"], reverse=True)
cos_rank = sorted(results.items(), key=lambda x: x[1]["Cosine"], reverse=True)
l2_rank = sorted(results.items(), key=lambda x: x[1]["L2"])

print(f"  IP:     {[x[0] for x in ip_rank]}")
print(f"  Cosine: {[x[0] for x in cos_rank]}")
print(f"  L2:     {[x[0] for x in l2_rank]}")

# ===== 3. 归一化的影响 =====
print("\n=== 归一化的影响 ===")

# 归一化后的文档
docs_norm = {k: normalize(v) for k, v in docs.items()}

print("归一化后的向量：")
for name, vec in docs_norm.items():
    print(f"  {name}: {vec}")

# 归一化后的内积 = 余弦相似度
print("\n验证：归一化后 IP = Cosine")
for name, vec in docs_norm.items():
    ip = inner_product(query, vec)
    cos = cosine_similarity(query, vec)
    print(f"  {name}: IP={ip:.4f}, Cosine={cos:.4f}, 差异={abs(ip-cos):.6f}")

# ===== 4. 向量数据库场景模拟 =====
print("\n=== 向量数据库场景模拟 ===")

# 模拟语义搜索（归一化embedding）
np.random.seed(42)

# 文档库（模拟BERT输出，已归一化）
n_docs = 1000
dim = 768
doc_embeddings = np.random.randn(n_docs, dim)
doc_embeddings = doc_embeddings / np.linalg.norm(doc_embeddings, axis=1, keepdims=True)

# 查询
query = np.random.randn(dim)
query = query / np.linalg.norm(query)

print(f"文档数: {n_docs}")
print(f"向量维度: {dim}")

# 三种度量的Top-5检索
import time

# 内积
start = time.time()
ip_scores = doc_embeddings @ query
ip_top5 = np.argsort(ip_scores)[-5:][::-1]
time_ip = time.time() - start

# 余弦（对归一化向量，结果应与IP相同）
start = time.time()
cos_scores = np.array([cosine_similarity(query, doc) for doc in doc_embeddings])
cos_top5 = np.argsort(cos_scores)[-5:][::-1]
time_cos = time.time() - start

# 欧氏距离
start = time.time()
l2_scores = np.linalg.norm(doc_embeddings - query, axis=1)
l2_top5 = np.argsort(l2_scores)[:5]
time_l2 = time.time() - start

print(f"\nTop-5结果：")
print(f"  IP:     {ip_top5.tolist()}")
print(f"  Cosine: {cos_top5.tolist()}")
print(f"  L2:     {l2_top5.tolist()}")

print(f"\n计算耗时：")
print(f"  IP:     {time_ip*1000:.2f}ms")
print(f"  Cosine: {time_cos*1000:.2f}ms")
print(f"  L2:     {time_l2*1000:.2f}ms")

# 验证归一化后IP和Cosine结果相同
print(f"\n归一化向量：IP Top-5 = Cosine Top-5? {np.array_equal(ip_top5, cos_top5)}")

# ===== 5. 度量选择决策函数 =====
print("\n=== 度量选择决策函数 ===")

def recommend_metric(normalized: bool, length_meaningful: bool, need_distance: bool) -> str:
    """
    根据数据特性推荐度量
    
    Args:
        normalized: embedding是否已归一化
        length_meaningful: 向量长度是否有业务含义
        need_distance: 是否需要真实空间距离
    
    Returns:
        推荐的度量名称
    """
    if normalized:
        return "IP (内积) - 最快，等价于余弦"
    
    if length_meaningful:
        return "IP (内积) - 保留长度信息"
    
    if need_distance:
        return "L2 (欧氏距离) - 真实空间距离"
    
    return "Cosine (余弦相似度) - 只看方向"

# 测试决策函数
scenarios = [
    {"name": "RAG检索（BERT embedding）", "normalized": True, "length_meaningful": False, "need_distance": False},
    {"name": "推荐系统（热门度+语义）", "normalized": False, "length_meaningful": True, "need_distance": False},
    {"name": "图像聚类", "normalized": False, "length_meaningful": False, "need_distance": True},
    {"name": "文本相似度（TF-IDF）", "normalized": False, "length_meaningful": False, "need_distance": False},
]

for s in scenarios:
    metric = recommend_metric(s["normalized"], s["length_meaningful"], s["need_distance"])
    print(f"  {s['name']}: {metric}")

# ===== 6. 三种度量的数学关系 =====
print("\n=== 三种度量的数学关系 ===")

# 对归一化向量：L2² = 2(1 - Cosine)
v1 = normalize(np.array([3, 4, 5]))
v2 = normalize(np.array([1, 2, 3]))

cos = cosine_similarity(v1, v2)
l2 = euclidean_distance(v1, v2)

print(f"余弦相似度: {cos:.6f}")
print(f"欧氏距离: {l2:.6f}")
print(f"L2² = {l2**2:.6f}")
print(f"2(1-cos) = {2*(1-cos):.6f}")
print(f"验证 L2² ≈ 2(1-cos): {np.isclose(l2**2, 2*(1-cos))}")

print("\n数学关系（归一化向量）：")
print("  L2² = 2(1 - Cosine)")
print("  Cosine = 1 - L2²/2")
print("  所以对归一化向量，L2和Cosine可以互相转换")

# ===== 7. 性能对比 =====
print("\n=== 批量计算性能对比 ===")

# 更大规模测试
n_docs = 100000
dim = 768

print(f"测试规模: {n_docs} 文档 × {dim} 维")

doc_matrix = np.random.randn(n_docs, dim).astype(np.float32)
doc_matrix = doc_matrix / np.linalg.norm(doc_matrix, axis=1, keepdims=True)
query = np.random.randn(dim).astype(np.float32)
query = query / np.linalg.norm(query)

# IP（矩阵乘法）
start = time.time()
ip_scores = doc_matrix @ query
time_ip = time.time() - start

# L2（向量化）
start = time.time()
diff = doc_matrix - query
l2_scores = np.sqrt(np.sum(diff * diff, axis=1))
time_l2 = time.time() - start

# Cosine（对归一化向量，等于IP）
start = time.time()
cos_scores = doc_matrix @ query  # 归一化后等于cosine
time_cos = time.time() - start

print(f"\n计算耗时：")
print(f"  IP (矩阵乘法):     {time_ip*1000:.2f}ms")
print(f"  L2 (向量化):       {time_l2*1000:.2f}ms")
print(f"  Cosine (归一化IP): {time_cos*1000:.2f}ms")

print("\n结论：")
print("  1. IP最快（只需一次矩阵乘法）")
print("  2. L2需要额外减法和平方根")
print("  3. 归一化向量用IP = 用Cosine（速度+准确）")
```

**运行输出示例：**
```
=== 三种度量的实现 ===
v1 = [3 4]
v2 = [6 8]
内积: 50
余弦: 1.0000
欧氏: 5.0000

=== 度量选择的影响 ===
查询: [1 0]

文档       IP         Cosine     L2
----------------------------------------
Doc_A      0.9500     0.9948     0.1414
Doc_B      10.0000    1.0000     9.0000
Doc_C      0.7000     0.7071     0.7211
Doc_D      0.5000     0.7071     0.7071

各度量的排名（最相似排第一）：
  IP:     ['Doc_B', 'Doc_A', 'Doc_C', 'Doc_D']
  Cosine: ['Doc_B', 'Doc_A', 'Doc_C', 'Doc_D']
  L2:     ['Doc_A', 'Doc_C', 'Doc_D', 'Doc_B']

=== 归一化的影响 ===
归一化后的向量：
  Doc_A: [0.99473684 0.10470914]
  Doc_B: [1. 0.]
  Doc_C: [0.70710678 0.70710678]
  Doc_D: [ 0.70710678 -0.70710678]

验证：归一化后 IP = Cosine
  Doc_A: IP=0.9947, Cosine=0.9947, 差异=0.000000
  Doc_B: IP=1.0000, Cosine=1.0000, 差异=0.000000
  Doc_C: IP=0.7071, Cosine=0.7071, 差异=0.000000
  Doc_D: IP=0.7071, Cosine=0.7071, 差异=0.000000

=== 度量选择决策函数 ===
  RAG检索（BERT embedding）: IP (内积) - 最快，等价于余弦
  推荐系统（热门度+语义）: IP (内积) - 保留长度信息
  图像聚类: L2 (欧氏距离) - 真实空间距离
  文本相似度（TF-IDF）: Cosine (余弦相似度) - 只看方向

=== 三种度量的数学关系 ===
余弦相似度: 0.978653
欧氏距离: 0.206559
L2² = 0.042667
2(1-cos) = 0.042693
验证 L2² ≈ 2(1-cos): True

数学关系（归一化向量）：
  L2² = 2(1 - Cosine)
  Cosine = 1 - L2²/2
  所以对归一化向量，L2和Cosine可以互相转换

=== 批量计算性能对比 ===
测试规模: 100000 文档 × 768 维

计算耗时：
  IP (矩阵乘法):     12.34ms
  L2 (向量化):       45.67ms
  Cosine (归一化IP): 12.35ms

结论：
  1. IP最快（只需一次矩阵乘法）
  2. L2需要额外减法和平方根
  3. 归一化向量用IP = 用Cosine（速度+准确）
```

---

---

## 8. 【面试必问】如果被问到，怎么答出彩

### 问题："向量数据库中如何选择相似度度量？"

**普通回答（❌ 不出彩）：**
"一般用余弦相似度，它不受向量长度影响。"

**出彩回答（✅ 推荐）：**

> **度量选择取决于三个因素：**
>
> 1. **embedding是否归一化？**
>    - 已归一化 → 用IP（内积），计算最快，结果等于余弦
>    - 未归一化 → 看下一个因素
>
> 2. **向量长度是否有业务含义？**
>    - 有（如热门度、重要性）→ 用IP，保留长度信息
>    - 无 → 用Cosine，只关注语义方向
>
> 3. **是否需要真实空间距离？**
>    - 是（如聚类、异常检测）→ 用L2（欧氏距离）
>    - 否 → 优先选Cosine或IP
>
> **实际建议：**
> - **RAG/语义搜索**：主流embedding模型（OpenAI、BERT）输出归一化向量，直接用IP
> - **推荐系统**：如需考虑热门度，用IP并保留原始长度
> - **聚类分析**：用L2，因为需要真实距离
>
> **性能考虑：**
> - IP最快（一次矩阵乘法）
> - L2较慢（需要减法+平方+开根号）
> - 归一化后IP=Cosine（兼顾速度和准确性）

**为什么这个回答出彩？**
1. ✅ 给出了决策框架，不是死记结论
2. ✅ 考虑了多个实际因素
3. ✅ 结合了具体应用场景
4. ✅ 提到了性能考虑

---

### 延伸问题："内积和余弦相似度什么时候结果相同？"

**出彩回答：**

> **数学关系：**
> ```
> Cosine(a,b) = IP(a,b) / (‖a‖ × ‖b‖)
> ```
>
> **结果相同的条件：** 当向量已归一化（‖a‖=‖b‖=1）时
> ```
> Cosine(a,b) = IP(a,b) / (1 × 1) = IP(a,b)
> ```
>
> **实际意义：**
> - 预处理时归一化所有向量
> - 查询时用IP度量
> - 得到的结果 = 余弦相似度结果
> - 但计算速度更快（省去除法）
>
> **这就是为什么向量数据库推荐"归一化+IP"的组合**

---

---

## 9. 【化骨绵掌】10个2分钟知识卡片

### 卡片1：三种度量一句话 🎯

**内积(IP)**：方向+长度，值越大越相似

**余弦(Cosine)**：只看方向，值越大越相似（[-1,1]）

**欧氏(L2)**：空间直线距离，值越小越相似

```python
import numpy as np

a = np.array([1, 2])
b = np.array([2, 4])

IP = np.dot(a, b)                    # 10
Cos = np.dot(a,b)/(np.linalg.norm(a)*np.linalg.norm(b))  # 1.0
L2 = np.linalg.norm(a - b)          # 2.24
```

**记忆口诀**：IP看全面，Cos看方向，L2看距离

---

### 卡片2：度量的值域 📊

| 度量 | 值域 | 最相似 | 最不相似 |
|-----|------|-------|---------|
| IP | (-∞,+∞) | +∞ | -∞ |
| Cosine | [-1,1] | 1 | -1 |
| L2 | [0,+∞) | 0 | +∞ |

**注意事项**：
- IP/Cosine：越大越相似
- L2：越小越相似（排序时要反向！）

---

### 卡片3：归一化的魔法 ✨

**核心公式**：
```
当 ‖a‖ = ‖b‖ = 1 时：
IP(a,b) = Cosine(a,b)
```

**代码验证**：
```python
def normalize(v):
    return v / np.linalg.norm(v)

a = normalize(np.array([3, 4]))
b = normalize(np.array([5, 12]))

print(np.dot(a, b))           # 0.8615
print(cosine_similarity(a,b)) # 0.8615
# 完全相同！
```

**意义**：归一化后用IP，既快又准

---

### 卡片4：IP和L2的关系 🔗

**对归一化向量**：
```
L2² = 2(1 - Cosine) = 2(1 - IP)
```

**推导**：
```
‖a-b‖² = ‖a‖² + ‖b‖² - 2(a·b)
       = 1 + 1 - 2(a·b)    # 归一化后
       = 2(1 - a·b)
       = 2(1 - Cosine)
```

**意义**：对归一化向量，IP排序和L2排序结果相同（只是顺序相反）

---

### 卡片5：快速选择流程 🚦

```
Q1: embedding归一化了吗？
    是 → 用IP（最快）
    否 → Q2

Q2: 长度有业务含义吗？
    是（热门度等）→ 用IP
    否 → Q3

Q3: 需要真实距离吗？
    是（聚类等）→ 用L2
    否 → 用Cosine
```

**默认推荐**：归一化 + IP

---

### 卡片6：向量数据库配置 ⚙️

**Milvus**：
```python
metric_type = "IP"      # 内积
metric_type = "COSINE"  # 余弦
metric_type = "L2"      # 欧氏距离
```

**Pinecone**：
```python
metric = "dotproduct"  # 内积
metric = "cosine"      # 余弦
metric = "euclidean"   # 欧氏距离
```

**Qdrant**：
```python
distance = Distance.DOT     # 内积
distance = Distance.COSINE  # 余弦
distance = Distance.EUCLID  # 欧氏距离
```

---

### 卡片7：性能对比 ⚡

**计算复杂度**：
- IP：O(n) 乘法 + O(n) 加法
- Cosine：IP + 2个范数计算 + 1除法
- L2：O(n) 减法 + O(n) 平方 + 1开根号

**实测速度**（10万×768维）：
```
IP:     ~15ms
Cosine: ~20ms（未优化）
L2:     ~50ms
```

**结论**：IP最快，L2最慢

---

### 卡片8：常见场景推荐 📋

| 场景 | 推荐 | 原因 |
|------|-----|------|
| RAG检索 | IP | embedding已归一化 |
| 语义搜索 | IP/Cosine | 关注语义方向 |
| 图像搜索 | Cosine | 不同大小的图片embedding长度可能不同 |
| 推荐系统 | IP | 可能需要保留热门度 |
| 文本聚类 | L2 | 需要真实距离做聚类 |
| 异常检测 | L2 | 检测距离异常的点 |

---

### 卡片9：长度信息的意义 📏

**什么时候长度有意义？**

1. **热门度加权**：
   ```python
   # 热门文档的embedding乘以权重
   doc_vec = embedding * popularity_score
   ```

2. **置信度表示**：
   ```python
   # 高置信度的embedding长度更大
   vec = model.encode(text) * confidence
   ```

3. **TF-IDF风格**：
   ```python
   # 词频影响向量长度
   vec = tf_idf_weights * word_vectors
   ```

**这些场景用IP而不是Cosine！**

---

### 卡片10：最佳实践总结 🏆

**推荐做法**：

1. **预处理时归一化**：
   ```python
   vec = vec / np.linalg.norm(vec)
   ```

2. **使用IP度量**：
   ```python
   metric_type = "IP"
   ```

3. **查询时也归一化**：
   ```python
   query = query / np.linalg.norm(query)
   ```

**这样做的好处**：
- ✅ 计算最快（只需矩阵乘法）
- ✅ 结果等于余弦相似度
- ✅ 存储空间可能更小（归一化后范围固定）
- ✅ 主流做法，兼容性好

---

---

## 10. 【一句话总结】

**度量选择决定了"相似"的定义：内积考虑方向和长度，余弦只看方向不受长度影响，欧氏距离衡量空间距离；对于语义搜索，推荐"预归一化+IP"的组合，兼顾准确性和性能。**

---

---

## 附录：快速参考卡 📋

### 三种度量速查

```python
import numpy as np

# 内积（越大越相似）
ip = np.dot(a, b)

# 余弦相似度（越大越相似，[-1,1]）
cos = np.dot(a, b) / (np.linalg.norm(a) * np.linalg.norm(b))

# 欧氏距离（越小越相似）
l2 = np.linalg.norm(a - b)

# 归一化
a_norm = a / np.linalg.norm(a)
```

### 度量选择决策表

| 条件 | 推荐度量 |
|------|---------|
| embedding已归一化 | IP |
| 长度有业务含义 | IP |
| 需要真实距离 | L2 |
| 只关心方向 | Cosine |
| 不确定 | 归一化+IP |

### 向量数据库配置速查

| 数据库 | IP | Cosine | L2 |
|-------|-----|--------|-----|
| Milvus | `"IP"` | `"COSINE"` | `"L2"` |
| Pinecone | `"dotproduct"` | `"cosine"` | `"euclidean"` |
| Qdrant | `Distance.DOT` | `Distance.COSINE` | `Distance.EUCLID` |
| Weaviate | `"dot"` | `"cosine"` | `"l2-squared"` |

### 学习检查清单 ✅

- [ ] 理解三种度量的计算公式
- [ ] 知道每种度量的值域和含义
- [ ] 理解归一化后IP=Cosine
- [ ] 能根据场景选择合适的度量
- [ ] 会在向量数据库中配置度量
- [ ] 理解维度灾难对L2的影响
- [ ] 知道"归一化+IP"的最佳实践

### 常见错误 ⚠️

| 错误 | 正确理解 |
|------|---------|
| 余弦总是最好的 | 取决于是否需要长度信息 |
| 三种度量效果差不多 | 可能导致完全不同的排名 |
| L2和Cosine差不多 | 度量的是完全不同的东西 |
| 不需要归一化 | 归一化后IP更快且等于Cosine |

### 下一步学习 🚀

掌握了度量选择后，建议学习：

1. **向量索引算法**：HNSW、IVF如何加速搜索
2. **向量量化**：PQ如何压缩存储
3. **混合搜索**：结合关键词和向量搜索
4. **评估指标**：如何衡量检索质量

---

## 参考资源 📚

1. **Milvus度量文档**：https://milvus.io/docs/metric.md
2. **Pinecone相似度指南**：https://docs.pinecone.io/docs/indexes#distance-metrics
3. **维度灾难论文**：Beyer et al., "When Is Nearest Neighbor Meaningful?"
4. **向量搜索实践**：https://www.pinecone.io/learn/vector-similarity/

---

**结语：** 度量选择看似简单，实则是向量数据库设计的核心决策。记住：没有最好的度量，只有最适合业务的度量。当不确定时，"归一化+IP"是一个安全的起点！💪