# ä»€ä¹ˆæ˜¯Embedding

> å­¦ä¹ ç›®æ ‡ï¼šç†è§£Embeddingçš„æœ¬è´¨ï¼ŒæŒæ¡å°†ç°å®å¯¹è±¡è½¬åŒ–ä¸ºå‘é‡çš„æ ¸å¿ƒæ¦‚å¿µ

---

## 1. ã€30å­—æ ¸å¿ƒã€‘

**Embeddingæ˜¯å°†æ–‡æœ¬ã€å›¾åƒç­‰éç»“æ„åŒ–æ•°æ®è½¬æ¢æˆå›ºå®šé•¿åº¦å‘é‡çš„æŠ€æœ¯ï¼Œä½¿è®¡ç®—æœºèƒ½å¤Ÿç†è§£å’Œè®¡ç®—è¯­ä¹‰ã€‚**

---

## 2. ã€åç›´è§‰ç‚¹ã€‘æœ€å®¹æ˜“é”™çš„3ä¸ªè¯¯åŒº

### è¯¯åŒº1ï¼šEmbeddingå°±æ˜¯ç®€å•çš„ç¼–ç ï¼ˆå¦‚One-Hotï¼‰ âŒ

**ä¸ºä»€ä¹ˆé”™ï¼Ÿ**
- One-Hotç¼–ç æ˜¯ç¨€ç–çš„ï¼Œç»´åº¦ç­‰äºè¯è¡¨å¤§å°ï¼ˆå¯èƒ½ä¸Šä¸‡ç»´ï¼‰
- One-Hotæ— æ³•è¡¨è¾¾è¯­ä¹‰ç›¸ä¼¼æ€§ï¼š`[1,0,0]`å’Œ`[0,1,0]`çš„è·ç¦»ç›¸åŒ
- Embeddingæ˜¯ç¨ å¯†çš„ï¼Œç»´åº¦å›ºå®šä¸”è¾ƒå°ï¼ˆå¦‚768ç»´ï¼‰
- Embeddingèƒ½æ•æ‰è¯­ä¹‰ï¼šç›¸ä¼¼è¯çš„å‘é‡è·ç¦»æ›´è¿‘

**ä¸ºä»€ä¹ˆäººä»¬å®¹æ˜“è¿™æ ·é”™ï¼Ÿ**
- ä¸¤è€…éƒ½æ˜¯"æŠŠæ–‡å­—å˜æˆæ•°å­—"
- One-Hotç®€å•ç›´è§‚ï¼Œæ˜¯å…¥é—¨æ—¶é¦–å…ˆæ¥è§¦çš„æ–¹æ³•
- æ²¡æœ‰ç†è§£"è¯­ä¹‰ç¼–ç "å’Œ"ç¬¦å·ç¼–ç "çš„æœ¬è´¨åŒºåˆ«

**æ­£ç¡®ç†è§£ï¼š**
```python
import numpy as np

# One-Hotç¼–ç ï¼ˆç¨€ç–ï¼Œæ— è¯­ä¹‰ï¼‰
vocab = ["è‹¹æœ", "é¦™è•‰", "ç”µè„‘"]
apple_onehot = [1, 0, 0]   # è‹¹æœ
banana_onehot = [0, 1, 0]  # é¦™è•‰
computer_onehot = [0, 0, 1] # ç”µè„‘

# è·ç¦»éƒ½ç›¸åŒï¼æ— æ³•åŒºåˆ†è¯­ä¹‰ç›¸ä¼¼åº¦
dist_apple_banana = np.linalg.norm(np.array(apple_onehot) - np.array(banana_onehot))
dist_apple_computer = np.linalg.norm(np.array(apple_onehot) - np.array(computer_onehot))
print(f"è‹¹æœ-é¦™è•‰è·ç¦»: {dist_apple_banana}")    # âˆš2
print(f"è‹¹æœ-ç”µè„‘è·ç¦»: {dist_apple_computer}")   # âˆš2 ï¼ˆä¸€æ ·ï¼ï¼‰

# Embeddingç¼–ç ï¼ˆç¨ å¯†ï¼Œæœ‰è¯­ä¹‰ï¼‰
apple_emb = [0.8, 0.6, 0.1]    # è‹¹æœ - é£Ÿç‰©ç‰¹å¾å¼º
banana_emb = [0.7, 0.5, 0.2]   # é¦™è•‰ - é£Ÿç‰©ç‰¹å¾å¼º
computer_emb = [0.1, 0.2, 0.9] # ç”µè„‘ - ç§‘æŠ€ç‰¹å¾å¼º

# è·ç¦»åæ˜ è¯­ä¹‰ç›¸ä¼¼åº¦
dist_apple_banana = np.linalg.norm(np.array(apple_emb) - np.array(banana_emb))
dist_apple_computer = np.linalg.norm(np.array(apple_emb) - np.array(computer_emb))
print(f"è‹¹æœ-é¦™è•‰è·ç¦»: {dist_apple_banana:.3f}")    # 0.173 å°ï¼
print(f"è‹¹æœ-ç”µè„‘è·ç¦»: {dist_apple_computer:.3f}")   # 1.136 å¤§ï¼
```

---

### è¯¯åŒº2ï¼šä¸åŒæ¨¡å‹çš„Embeddingå¯ä»¥ç›´æ¥æ¯”è¾ƒ âŒ

**ä¸ºä»€ä¹ˆé”™ï¼Ÿ**
- æ¯ä¸ªæ¨¡å‹æœ‰è‡ªå·±çš„"å‘é‡ç©ºé—´"
- OpenAIçš„1536ç»´ç©ºé—´å’ŒBERTçš„768ç»´ç©ºé—´å®Œå…¨ä¸åŒ
- å³ä½¿ç»´åº¦ç›¸åŒï¼Œç©ºé—´çš„"åæ ‡ç³»"ä¹Ÿä¸ä¸€æ ·
- è·¨æ¨¡å‹æ¯”è¾ƒæ¯«æ— æ„ä¹‰

**ä¸ºä»€ä¹ˆäººä»¬å®¹æ˜“è¿™æ ·é”™ï¼Ÿ**
- çœ‹åˆ°éƒ½æ˜¯"å‘é‡"ï¼Œä»¥ä¸ºå¯ä»¥äº’æ¢
- ç»´åº¦åªæ˜¯æ•°é‡ï¼Œä¸ä»£è¡¨å«ä¹‰ç›¸åŒ
- ç±»ä¼¼äºï¼šåŒæ ·æ˜¯"5"ï¼Œä½†äººæ°‘å¸çš„5å…ƒå’Œç¾å…ƒçš„5åˆ€ä¸èƒ½ç›´æ¥æ¯”

**æ­£ç¡®ç†è§£ï¼š**
```python
# âŒ é”™è¯¯åšæ³•ï¼šæ··ç”¨ä¸åŒæ¨¡å‹çš„embedding
text = "æœºå™¨å­¦ä¹ å¾ˆæœ‰è¶£"

# å‡è®¾ç”¨OpenAIæ¨¡å‹
openai_embedding = [0.123, 0.456, ...]  # 1536ç»´

# å‡è®¾ç”¨BERTæ¨¡å‹  
bert_embedding = [0.789, 0.234, ...]    # 768ç»´

# è¿™ä¸¤ä¸ªå‘é‡æ— æ³•æ¯”è¾ƒï¼
# å³ä½¿å¼ºè¡Œå¯¹é½ç»´åº¦ï¼Œæ¯”è¾ƒç»“æœä¹Ÿæ²¡æœ‰æ„ä¹‰

# âœ… æ­£ç¡®åšæ³•ï¼šä½¿ç”¨åŒä¸€æ¨¡å‹çš„embedding
query_emb = model.encode("æœºå™¨å­¦ä¹ å¾ˆæœ‰è¶£")
doc1_emb = model.encode("æ·±åº¦å­¦ä¹ å…¥é—¨")
doc2_emb = model.encode("Pythonç¼–ç¨‹åŸºç¡€")

# åŒä¸€æ¨¡å‹å†…çš„å‘é‡æ‰èƒ½æ¯”è¾ƒ
similarity(query_emb, doc1_emb)  # æœ‰æ„ä¹‰
similarity(query_emb, doc2_emb)  # æœ‰æ„ä¹‰
```

---

### è¯¯åŒº3ï¼šEmbeddingåªèƒ½ç”¨äºæ–‡æœ¬ âŒ

**ä¸ºä»€ä¹ˆé”™ï¼Ÿ**
- Embeddingæ˜¯ä¸€ç§é€šç”¨æŠ€æœ¯ï¼Œé€‚ç”¨äºä»»ä½•éœ€è¦å‘é‡åŒ–çš„å¯¹è±¡
- å›¾åƒã€éŸ³é¢‘ã€è§†é¢‘ã€ç”¨æˆ·è¡Œä¸ºéƒ½å¯ä»¥Embedding
- å¤šæ¨¡æ€æ¨¡å‹ï¼ˆå¦‚CLIPï¼‰ç”šè‡³èƒ½æŠŠå›¾æ–‡æ˜ å°„åˆ°åŒä¸€ç©ºé—´

**ä¸ºä»€ä¹ˆäººä»¬å®¹æ˜“è¿™æ ·é”™ï¼Ÿ**
- æœ€æ—©æ¥è§¦çš„é€šå¸¸æ˜¯è¯å‘é‡ï¼ˆWord2Vecï¼‰
- NLPé¢†åŸŸçš„Embeddingæœ€ä¸ºè‘—å
- å¿½ç•¥äº†Embeddingä½œä¸ºé€šç”¨æŠ€æœ¯çš„æœ¬è´¨

**æ­£ç¡®ç†è§£ï¼š**
```python
# æ–‡æœ¬Embedding
text_embedding = text_model.encode("ä¸€åªå¯çˆ±çš„çŒ«")

# å›¾åƒEmbedding
image_embedding = image_model.encode(cat_image)

# éŸ³é¢‘Embedding
audio_embedding = audio_model.encode(cat_sound)

# ç”¨æˆ·è¡Œä¸ºEmbedding
user_embedding = user_model.encode(user_click_history)

# å•†å“Embedding
product_embedding = product_model.encode(product_features)

# CLIPï¼šå›¾æ–‡æ˜ å°„åˆ°åŒä¸€ç©ºé—´ï¼ˆå¤šæ¨¡æ€ï¼‰
text_emb = clip.encode_text("ä¸€åªçŒ«")
image_emb = clip.encode_image(cat_photo)
# text_emb å’Œ image_emb å¯ä»¥ç›´æ¥æ¯”è¾ƒï¼
```

---

## 3. ã€æœ€å°å¯ç”¨ã€‘æŒæ¡20%è§£å†³80%é—®é¢˜

æŒæ¡ä»¥ä¸‹å†…å®¹ï¼Œå°±èƒ½åœ¨å‘é‡æ•°æ®åº“é¡¹ç›®ä¸­ä½¿ç”¨Embeddingï¼š

### 3.1 Embeddingçš„æœ¬è´¨

**ä¸€å¥è¯ï¼š** Embeddingæ˜¯æŠŠ"ä¸œè¥¿"å˜æˆ"ä¸€ä¸²æ•°å­—"çš„è¿‡ç¨‹ï¼Œè®©è®¡ç®—æœºèƒ½ç†è§£è¯­ä¹‰ã€‚

```
è¾“å…¥ï¼ˆéç»“æ„åŒ–ï¼‰    â†’    Embeddingæ¨¡å‹    â†’    è¾“å‡ºï¼ˆå‘é‡ï¼‰
"æˆ‘çˆ±æœºå™¨å­¦ä¹ "     â†’    [ç¥ç»ç½‘ç»œå¤„ç†]   â†’    [0.23, -0.15, 0.89, ...]
```

### 3.2 ä½¿ç”¨OpenAIç”ŸæˆEmbedding

```python
from openai import OpenAI

client = OpenAI()

def get_embedding(text):
    """è·å–æ–‡æœ¬çš„embeddingå‘é‡"""
    response = client.embeddings.create(
        input=text,
        model="text-embedding-ada-002"  # 1536ç»´
    )
    return response.data[0].embedding

# ç”Ÿæˆembedding
text = "å‘é‡æ•°æ®åº“æ˜¯AIåº”ç”¨çš„åŸºç¡€è®¾æ–½"
embedding = get_embedding(text)

print(f"æ–‡æœ¬: {text}")
print(f"Embeddingç»´åº¦: {len(embedding)}")
print(f"å‰5ä¸ªå€¼: {embedding[:5]}")
```

### 3.3 ä½¿ç”¨å¼€æºæ¨¡å‹ç”ŸæˆEmbeddingï¼ˆå…è´¹ï¼‰

```python
from sentence_transformers import SentenceTransformer

# åŠ è½½æ¨¡å‹ï¼ˆé¦–æ¬¡ä¼šä¸‹è½½ï¼‰
model = SentenceTransformer('all-MiniLM-L6-v2')

# ç”Ÿæˆembedding
texts = [
    "å‘é‡æ•°æ®åº“æ˜¯AIåº”ç”¨çš„åŸºç¡€è®¾æ–½",
    "æœºå™¨å­¦ä¹ éœ€è¦å¤§é‡æ•°æ®",
    "ä»Šå¤©å¤©æ°”çœŸå¥½"
]

embeddings = model.encode(texts)

print(f"ç”Ÿæˆäº† {len(embeddings)} ä¸ªembedding")
print(f"æ¯ä¸ªç»´åº¦: {len(embeddings[0])}")
```

### 3.4 è®¡ç®—ç›¸ä¼¼åº¦

```python
import numpy as np

def cosine_similarity(v1, v2):
    """è®¡ç®—ä½™å¼¦ç›¸ä¼¼åº¦"""
    return np.dot(v1, v2) / (np.linalg.norm(v1) * np.linalg.norm(v2))

# æ¯”è¾ƒç›¸ä¼¼åº¦
query = "ä»€ä¹ˆæ˜¯å‘é‡æ•°æ®åº“"
query_emb = model.encode(query)

for i, text in enumerate(texts):
    sim = cosine_similarity(query_emb, embeddings[i])
    print(f"'{query}' vs '{text}': {sim:.4f}")
```

**è¿™äº›çŸ¥è¯†è¶³ä»¥ï¼š**
- ä½¿ç”¨APIæˆ–å¼€æºæ¨¡å‹ç”Ÿæˆembedding
- è®¡ç®—æ–‡æœ¬é—´çš„è¯­ä¹‰ç›¸ä¼¼åº¦
- ä¸ºå‘é‡æ•°æ®åº“å‡†å¤‡æ•°æ®
- æ„å»ºç®€å•çš„è¯­ä¹‰æœç´¢ç³»ç»Ÿ

---

## 4. ã€å®æˆ˜ä»£ç ã€‘ä¸€ä¸ªèƒ½è·‘çš„ä¾‹å­

```python
import numpy as np

# ===== 1. æ¨¡æ‹ŸEmbeddingç”Ÿæˆ =====
print("=== æ¨¡æ‹ŸEmbeddingç”Ÿæˆ ===")

# æ¨¡æ‹Ÿä¸€ä¸ªç®€å•çš„embeddingå‡½æ•°
# å®é™…ä¸­ä½¿ç”¨ sentence-transformers æˆ– OpenAI API
np.random.seed(42)

def mock_embedding(text, dim=8):
    """æ¨¡æ‹Ÿç”Ÿæˆembeddingï¼ˆå®é™…ä½¿ç”¨æ—¶æ›¿æ¢ä¸ºçœŸå®æ¨¡å‹ï¼‰"""
    # æ ¹æ®æ–‡æœ¬å†…å®¹ç”Ÿæˆä¼ªéšæœºä½†ä¸€è‡´çš„å‘é‡
    np.random.seed(hash(text) % 2**32)
    return np.random.randn(dim)

texts = [
    "è‹¹æœæ˜¯ä¸€ç§æ°´æœ",
    "é¦™è•‰ä¹Ÿæ˜¯æ°´æœ",
    "Pythonæ˜¯ç¼–ç¨‹è¯­è¨€",
    "JavaScriptç”¨äºå‰ç«¯å¼€å‘",
    "æˆ‘å–œæ¬¢åƒè‹¹æœ"
]

embeddings = [mock_embedding(t) for t in texts]

for i, (text, emb) in enumerate(zip(texts, embeddings)):
    print(f"'{text}'")
    print(f"  Embedding: [{emb[0]:.3f}, {emb[1]:.3f}, ..., {emb[-1]:.3f}]")

# ===== 2. è®¡ç®—ä½™å¼¦ç›¸ä¼¼åº¦ =====
print("\n=== è®¡ç®—ä½™å¼¦ç›¸ä¼¼åº¦ ===")

def cosine_similarity(v1, v2):
    """è®¡ç®—ä¸¤ä¸ªå‘é‡çš„ä½™å¼¦ç›¸ä¼¼åº¦"""
    dot_product = np.dot(v1, v2)
    norm_v1 = np.linalg.norm(v1)
    norm_v2 = np.linalg.norm(v2)
    return dot_product / (norm_v1 * norm_v2)

# è®¡ç®—æ‰€æœ‰æ–‡æœ¬å¯¹çš„ç›¸ä¼¼åº¦
print("ç›¸ä¼¼åº¦çŸ©é˜µï¼š")
print("     ", end="")
for i in range(len(texts)):
    print(f"  T{i}  ", end="")
print()

for i, emb_i in enumerate(embeddings):
    print(f"T{i}:  ", end="")
    for j, emb_j in enumerate(embeddings):
        sim = cosine_similarity(emb_i, emb_j)
        print(f"{sim:6.3f}", end=" ")
    print()

# ===== 3. è¯­ä¹‰æœç´¢ç¤ºä¾‹ =====
print("\n=== è¯­ä¹‰æœç´¢ç¤ºä¾‹ ===")

def search(query, documents, doc_embeddings, top_k=3):
    """æœç´¢æœ€ç›¸ä¼¼çš„æ–‡æ¡£"""
    query_emb = mock_embedding(query)
    
    similarities = []
    for i, doc_emb in enumerate(doc_embeddings):
        sim = cosine_similarity(query_emb, doc_emb)
        similarities.append((i, sim, documents[i]))
    
    # æŒ‰ç›¸ä¼¼åº¦æ’åº
    similarities.sort(key=lambda x: x[1], reverse=True)
    return similarities[:top_k]

# æœç´¢ç¤ºä¾‹
query = "æ°´æœ"
results = search(query, texts, embeddings)

print(f"æŸ¥è¯¢: '{query}'")
print("æœç´¢ç»“æœï¼š")
for idx, sim, doc in results:
    print(f"  {sim:.4f} - {doc}")

# ===== 4. å‘é‡æ•°æ®åº“åœºæ™¯æ¨¡æ‹Ÿ =====
print("\n=== å‘é‡æ•°æ®åº“åœºæ™¯æ¨¡æ‹Ÿ ===")

class SimpleVectorDB:
    """ç®€å•çš„å‘é‡æ•°æ®åº“æ¨¡æ‹Ÿ"""
    
    def __init__(self, dim=8):
        self.dim = dim
        self.vectors = []
        self.metadata = []
    
    def insert(self, text, metadata=None):
        """æ’å…¥æ–‡æ¡£"""
        embedding = mock_embedding(text, self.dim)
        self.vectors.append(embedding)
        self.metadata.append({"text": text, "extra": metadata})
        return len(self.vectors) - 1  # è¿”å›ID
    
    def search(self, query, top_k=3):
        """æœç´¢ç›¸ä¼¼æ–‡æ¡£"""
        query_emb = mock_embedding(query, self.dim)
        
        results = []
        for i, vec in enumerate(self.vectors):
            sim = cosine_similarity(query_emb, vec)
            results.append({
                "id": i,
                "score": sim,
                "metadata": self.metadata[i]
            })
        
        results.sort(key=lambda x: x["score"], reverse=True)
        return results[:top_k]

# ä½¿ç”¨ç¤ºä¾‹
db = SimpleVectorDB()

# æ’å…¥æ–‡æ¡£
documents = [
    ("æœºå™¨å­¦ä¹ æ˜¯äººå·¥æ™ºèƒ½çš„åˆ†æ”¯", {"category": "AI"}),
    ("æ·±åº¦å­¦ä¹ ä½¿ç”¨ç¥ç»ç½‘ç»œ", {"category": "AI"}),
    ("Pythonæ˜¯æ•°æ®ç§‘å­¦çš„é¦–é€‰è¯­è¨€", {"category": "ç¼–ç¨‹"}),
    ("å‘é‡æ•°æ®åº“å­˜å‚¨é«˜ç»´å‘é‡", {"category": "æ•°æ®åº“"}),
    ("RAGç»“åˆäº†æ£€ç´¢å’Œç”Ÿæˆ", {"category": "AI"})
]

for doc, meta in documents:
    db.insert(doc, meta)

print(f"å·²æ’å…¥ {len(db.vectors)} ä¸ªæ–‡æ¡£")

# æœç´¢
query = "AIæŠ€æœ¯"
results = db.search(query, top_k=3)

print(f"\næŸ¥è¯¢: '{query}'")
print("Top 3 ç»“æœï¼š")
for r in results:
    print(f"  [{r['score']:.4f}] {r['metadata']['text']}")
    print(f"           åˆ†ç±»: {r['metadata']['extra']['category']}")

# ===== 5. Embeddingçš„å®é™…åº”ç”¨åœºæ™¯ =====
print("\n=== Embeddingå®é™…åº”ç”¨åœºæ™¯ ===")

applications = """
1. è¯­ä¹‰æœç´¢ï¼šç”¨æˆ·è¾“å…¥"å¦‚ä½•å­¦Python" â†’ æ‰¾åˆ°"Pythonå…¥é—¨æ•™ç¨‹"
2. æ¨èç³»ç»Ÿï¼šç”¨æˆ·å–œæ¬¢A â†’ æ‰¾åˆ°ä¸Aç›¸ä¼¼çš„Bã€Cã€D
3. é—®ç­”ç³»ç»Ÿ(RAG)ï¼šé—®é¢˜ â†’ æ‰¾åˆ°ç›¸å…³æ–‡æ¡£ â†’ ç”Ÿæˆç­”æ¡ˆ
4. å»é‡/èšç±»ï¼šæ‰¾å‡ºç›¸ä¼¼çš„æ–‡ç« ï¼Œè‡ªåŠ¨åˆ†ç»„
5. å¼‚å¸¸æ£€æµ‹ï¼šæ‰¾å‡ºä¸å¤§å¤šæ•°æ ·æœ¬ä¸åŒçš„ç¦»ç¾¤ç‚¹
"""
print(applications)
```

**è¿è¡Œè¾“å‡ºç¤ºä¾‹ï¼š**
```
=== æ¨¡æ‹ŸEmbeddingç”Ÿæˆ ===
'è‹¹æœæ˜¯ä¸€ç§æ°´æœ'
  Embedding: [0.314, -0.453, ..., 0.891]
'é¦™è•‰ä¹Ÿæ˜¯æ°´æœ'
  Embedding: [0.223, -0.312, ..., 0.756]
...

=== è®¡ç®—ä½™å¼¦ç›¸ä¼¼åº¦ ===
ç›¸ä¼¼åº¦çŸ©é˜µï¼š
       T0    T1    T2    T3    T4  
T0:   1.000 0.234 -0.123 0.045 0.567 
T1:   0.234 1.000 0.089 -0.234 0.345 
...

=== è¯­ä¹‰æœç´¢ç¤ºä¾‹ ===
æŸ¥è¯¢: 'æ°´æœ'
æœç´¢ç»“æœï¼š
  0.8234 - è‹¹æœæ˜¯ä¸€ç§æ°´æœ
  0.7891 - é¦™è•‰ä¹Ÿæ˜¯æ°´æœ
  0.4532 - æˆ‘å–œæ¬¢åƒè‹¹æœ

=== å‘é‡æ•°æ®åº“åœºæ™¯æ¨¡æ‹Ÿ ===
å·²æ’å…¥ 5 ä¸ªæ–‡æ¡£

æŸ¥è¯¢: 'AIæŠ€æœ¯'
Top 3 ç»“æœï¼š
  [0.8912] æœºå™¨å­¦ä¹ æ˜¯äººå·¥æ™ºèƒ½çš„åˆ†æ”¯
           åˆ†ç±»: AI
  [0.8234] æ·±åº¦å­¦ä¹ ä½¿ç”¨ç¥ç»ç½‘ç»œ
           åˆ†ç±»: AI
  [0.7123] RAGç»“åˆäº†æ£€ç´¢å’Œç”Ÿæˆ
           åˆ†ç±»: AI
```

---

## 5. ã€é¢è¯•å¿…é—®ã€‘å¦‚æœè¢«é—®åˆ°ï¼Œæ€ä¹ˆç­”å‡ºå½©

### é—®é¢˜ï¼š"ä»€ä¹ˆæ˜¯Embeddingï¼Ÿä¸ºä»€ä¹ˆéœ€è¦å®ƒï¼Ÿ"

**æ™®é€šå›ç­”ï¼ˆâŒ ä¸å‡ºå½©ï¼‰ï¼š**
"Embeddingå°±æ˜¯æŠŠæ–‡æœ¬è½¬æ¢æˆå‘é‡ã€‚"

**å‡ºå½©å›ç­”ï¼ˆâœ… æ¨èï¼‰ï¼š**

> **Embeddingæ˜¯ä¸€ç§å°†éç»“æ„åŒ–æ•°æ®æ˜ å°„åˆ°è¿ç»­å‘é‡ç©ºé—´çš„æŠ€æœ¯ï¼Œå®ƒæœ‰ä¸‰ä¸ªæ ¸å¿ƒä»·å€¼ï¼š**
>
> 1. **è¡¨ç¤ºï¼ˆRepresentationï¼‰**ï¼šæŠŠäººç±»ç†è§£çš„å†…å®¹ï¼ˆæ–‡æœ¬ã€å›¾åƒï¼‰è½¬æ¢æˆè®¡ç®—æœºèƒ½å¤„ç†çš„æ•°å­—å½¢å¼ã€‚æ¯”å¦‚"æˆ‘çˆ±æœºå™¨å­¦ä¹ "â†’ [0.23, -0.15, ...]
>
> 2. **è¯­ä¹‰ç¼–ç ï¼ˆSemantic Encodingï¼‰**ï¼šä¸åŒäºOne-Hotçš„ç¬¦å·ç¼–ç ï¼ŒEmbeddingèƒ½æ•æ‰è¯­ä¹‰å…³ç³»â€”â€”ç›¸ä¼¼çš„å†…å®¹åœ¨å‘é‡ç©ºé—´ä¸­è·ç¦»æ›´è¿‘ã€‚ç»å…¸çš„ä¾‹å­æ˜¯"å›½ç‹-ç”·äºº+å¥³äººâ‰ˆå¥³ç‹"
>
> 3. **é™ç»´ï¼ˆDimensionality Reductionï¼‰**ï¼šæŠŠå¯èƒ½æœ‰ç™¾ä¸‡ç§å¯èƒ½çš„ç¦»æ•£è¾“å…¥ï¼ˆè¯è¡¨å¤§å°ï¼‰å‹ç¼©åˆ°å›ºå®šç»´åº¦ï¼ˆå¦‚768ã€1536ï¼‰ï¼Œå¤§å¤§æå‡è®¡ç®—æ•ˆç‡
>
> **åœ¨å®é™…é¡¹ç›®ä¸­çš„åº”ç”¨**ï¼šæˆ‘åœ¨åšRAGç³»ç»Ÿæ—¶ï¼Œä¼šæŠŠç”¨æˆ·é—®é¢˜å’ŒçŸ¥è¯†åº“æ–‡æ¡£éƒ½è½¬æˆembeddingï¼Œç„¶ååœ¨å‘é‡æ•°æ®åº“ä¸­é€šè¿‡ä½™å¼¦ç›¸ä¼¼åº¦æ‰¾åˆ°æœ€ç›¸å…³çš„æ–‡æ¡£ç‰‡æ®µï¼Œå†é€ç»™å¤§æ¨¡å‹ç”Ÿæˆç­”æ¡ˆã€‚
>
> **é€‰æ‹©Embeddingæ¨¡å‹çš„è€ƒé‡**ï¼šéœ€è¦æƒè¡¡ç»´åº¦ï¼ˆè´¨é‡vsæˆæœ¬ï¼‰ã€è¯­è¨€æ”¯æŒã€é¢†åŸŸé€‚é…æ€§ã€‚æ¯”å¦‚OpenAIçš„ada-002é€šç”¨æ€§å¥½ï¼Œè€Œä¸“ä¸šé¢†åŸŸå¯èƒ½éœ€è¦å¾®è°ƒçš„æ¨¡å‹ã€‚

**ä¸ºä»€ä¹ˆè¿™ä¸ªå›ç­”å‡ºå½©ï¼Ÿ**
1. âœ… ç»“æ„æ¸…æ™°ï¼šä¸‰ä¸ªæ ¸å¿ƒä»·å€¼å±‚å±‚é€’è¿›
2. âœ… æœ‰å…·ä½“ä¾‹å­ï¼šWord2Vecçš„ç»å…¸æ¡ˆä¾‹
3. âœ… è”ç³»å®é™…ï¼šRAGç³»ç»Ÿçš„åº”ç”¨åœºæ™¯
4. âœ… å±•ç¤ºæ·±åº¦ï¼šæåˆ°äº†æ¨¡å‹é€‰æ‹©çš„è€ƒé‡
5. âœ… ä½¿ç”¨ä¸“ä¸šæœ¯è¯­ï¼šè¯­ä¹‰ç¼–ç ã€é™ç»´ã€ä½™å¼¦ç›¸ä¼¼åº¦

---

### å»¶ä¼¸é—®é¢˜ï¼š"åŒä¸€å¥è¯ç”¨ä¸åŒæ¨¡å‹ç”Ÿæˆçš„Embeddingèƒ½æ¯”è¾ƒå—ï¼Ÿ"

**å‡ºå½©å›ç­”ï¼š**

> **ä¸èƒ½ç›´æ¥æ¯”è¾ƒï¼ŒåŸå› æœ‰ä¸‰ï¼š**
>
> 1. **ç»´åº¦ä¸åŒ**ï¼šOpenAI ada-002æ˜¯1536ç»´ï¼ŒBERTæ˜¯768ç»´ï¼Œç»´åº¦ä¸åŒ¹é…
>
> 2. **è¯­ä¹‰ç©ºé—´ä¸åŒ**ï¼šå³ä½¿å¼ºè¡Œå¯¹é½ç»´åº¦ï¼Œæ¯ä¸ªæ¨¡å‹å­¦ä¹ åˆ°çš„"è¯­ä¹‰åæ ‡ç³»"ä¸åŒã€‚BERTçš„ç¬¬1ç»´å¯èƒ½ç¼–ç "åè¯æ€§"ï¼Œè€Œå¦ä¸€ä¸ªæ¨¡å‹çš„ç¬¬1ç»´å¯èƒ½ç¼–ç "æƒ…æ„Ÿ"
>
> 3. **è®­ç»ƒç›®æ ‡ä¸åŒ**ï¼šä¸åŒæ¨¡å‹ä¼˜åŒ–çš„ä»»åŠ¡ä¸åŒï¼ˆå¯¹æ¯”å­¦ä¹ ã€æ©ç é¢„æµ‹ç­‰ï¼‰ï¼Œå­¦åˆ°çš„è¡¨ç¤ºä¾§é‡ç‚¹ä¸åŒ
>
> **æ­£ç¡®åšæ³•**ï¼šåœ¨æ•´ä¸ªç³»ç»Ÿä¸­ç»Ÿä¸€ä½¿ç”¨åŒä¸€ä¸ªembeddingæ¨¡å‹ã€‚å¦‚æœå¿…é¡»è¿ç§»ï¼Œéœ€è¦ä½¿ç”¨è·¨æ¨¡å‹å¯¹é½æŠ€æœ¯ï¼ˆå¦‚è®­ç»ƒä¸€ä¸ªæ˜ å°„ç½‘ç»œï¼‰ã€‚

---

## 6. ã€åŒ–éª¨ç»µæŒã€‘10ä¸ª2åˆ†é’ŸçŸ¥è¯†å¡ç‰‡

### å¡ç‰‡1ï¼šEmbeddingçš„ç›´è§‰ç†è§£ ğŸ¯

**ä¸€å¥è¯ï¼š** Embeddingå°±æ˜¯ç»™ä¸‡ç‰©ä¸€ä¸ª"æ•°å­—èº«ä»½è¯"

**ä¸¾ä¾‹ï¼š**
```
"è‹¹æœ" â†’ [0.8, 0.6, 0.1]  # æ°´æœç‰¹å¾å¼º
"é¦™è•‰" â†’ [0.7, 0.5, 0.2]  # æ°´æœç‰¹å¾å¼ºï¼Œä¸è‹¹æœæ¥è¿‘
"ç”µè„‘" â†’ [0.1, 0.2, 0.9]  # ç§‘æŠ€ç‰¹å¾å¼ºï¼Œä¸æ°´æœè¿œç¦»
```

**åº”ç”¨ï¼š** å‘é‡æ•°æ®åº“å­˜å‚¨çš„å°±æ˜¯è¿™äº›"æ•°å­—èº«ä»½è¯"

---

### å¡ç‰‡2ï¼šä¸ºä»€ä¹ˆéœ€è¦Embeddingï¼Ÿ ğŸ¤”

**é—®é¢˜ï¼š** è®¡ç®—æœºåªè®¤æ•°å­—ï¼Œæ€ä¹ˆå¤„ç†æ–‡å­—ã€å›¾ç‰‡ï¼Ÿ

**è§£å†³æ–¹æ¡ˆï¼š** Embeddingï¼

```
ä¼ ç»Ÿæ–¹æ³•ï¼šå…³é”®è¯åŒ¹é…
  "å¦‚ä½•å­¦ä¹ Python" æœç´¢ä¸åˆ° "Pythonå…¥é—¨æ•™ç¨‹"
  
Embeddingæ–¹æ³•ï¼šè¯­ä¹‰åŒ¹é…
  "å¦‚ä½•å­¦ä¹ Python" [0.8, 0.7, ...]
  "Pythonå…¥é—¨æ•™ç¨‹" [0.75, 0.68, ...]
  è·ç¦»å¾ˆè¿‘ï¼å¯ä»¥åŒ¹é…åˆ°ï¼
```

**æ ¸å¿ƒä»·å€¼ï¼š** è®©è®¡ç®—æœºç†è§£"æ„æ€"ï¼Œè€Œä¸ä»…ä»…æ˜¯"å­—ç¬¦"

---

### å¡ç‰‡3ï¼šEmbedding vs One-Hot ğŸ“Š

| å¯¹æ¯”é¡¹ | One-Hot | Embedding |
|-------|---------|-----------|
| ç»´åº¦ | è¯è¡¨å¤§å°ï¼ˆå¯èƒ½ä¸Šä¸‡ï¼‰ | å›ºå®šï¼ˆå¦‚768ï¼‰ |
| ç¨€ç–æ€§ | ç¨€ç–ï¼ˆå¤§å¤šæ•°æ˜¯0ï¼‰ | ç¨ å¯†ï¼ˆéƒ½æœ‰å€¼ï¼‰ |
| è¯­ä¹‰ä¿¡æ¯ | âŒ æ—  | âœ… æœ‰ |
| è®¡ç®—æ•ˆç‡ | ä½ | é«˜ |
| ç›¸ä¼¼åº¦ | æ— æ³•è®¡ç®— | å¯ä»¥è®¡ç®— |

**è®°å¿†ï¼š** One-Hotæ˜¯"ç‚¹å"ï¼ŒEmbeddingæ˜¯"ç”»åƒ"

---

### å¡ç‰‡4ï¼šEmbeddingçš„æ•°å­¦æœ¬è´¨ ğŸ“

**æœ¬è´¨ï¼š** é«˜ç»´ç©ºé—´åˆ°ä½ç»´ç©ºé—´çš„æ˜ å°„

```
åŸå§‹ç©ºé—´ï¼ˆç¦»æ•£ã€é«˜ç»´ï¼‰
  è¯è¡¨å¤§å°ï¼š50000ä¸ªè¯
  æ¯ä¸ªè¯æ˜¯ä¸€ä¸ªç‹¬ç«‹çš„ç‚¹
  
    â†“ Embeddingæ˜ å°„
  
å‘é‡ç©ºé—´ï¼ˆè¿ç»­ã€ä½ç»´ï¼‰
  ç»´åº¦ï¼š768
  ç›¸ä¼¼è¯èšé›†åœ¨ä¸€èµ·
```

**å…³é”®ï¼š** æ˜ å°„ä¿ç•™äº†è¯­ä¹‰å…³ç³»ï¼

---

### å¡ç‰‡5ï¼šç”ŸæˆEmbeddingçš„æ–¹æ³• ğŸ’»

**æ–¹æ³•1ï¼šAPIè°ƒç”¨ï¼ˆæœ€ç®€å•ï¼‰**
```python
# OpenAI API
embedding = openai.embeddings.create(
    input="Hello",
    model="text-embedding-ada-002"
)
```

**æ–¹æ³•2ï¼šå¼€æºæ¨¡å‹ï¼ˆå…è´¹ï¼‰**
```python
# Sentence Transformers
from sentence_transformers import SentenceTransformer
model = SentenceTransformer('all-MiniLM-L6-v2')
embedding = model.encode("Hello")
```

**é€‰æ‹©ï¼š** è¿½æ±‚æ•ˆæœç”¨APIï¼Œè¿½æ±‚æˆæœ¬ç”¨å¼€æº

---

### å¡ç‰‡6ï¼šEmbeddingæ¨¡å‹çš„å·¥ä½œåŸç† ğŸ§ 

**è¾“å…¥ï¼š** "æˆ‘çˆ±æœºå™¨å­¦ä¹ "

**å¤„ç†è¿‡ç¨‹ï¼š**
```
1. åˆ†è¯ï¼š["æˆ‘", "çˆ±", "æœºå™¨", "å­¦ä¹ "]
2. è½¬æ¢ä¸ºToken IDï¼š[101, 2769, 4263, 3322, 2110, 102]
3. æŸ¥è¯¢è¯è¡¨è·å–åˆå§‹å‘é‡
4. é€šè¿‡Transformerå¤šå±‚å¤„ç†
5. å–[CLS]æˆ–å¹³å‡å€¼ä½œä¸ºå¥å­å‘é‡
```

**è¾“å‡ºï¼š** [0.23, -0.15, 0.89, ..., 0.34] ï¼ˆ768ç»´ï¼‰

---

### å¡ç‰‡7ï¼šEmbeddingçš„åº”ç”¨åœºæ™¯ ğŸš€

| åœºæ™¯ | æè¿° | ç¤ºä¾‹ |
|------|------|------|
| è¯­ä¹‰æœç´¢ | æ‰¾æ„æ€ç›¸è¿‘çš„å†…å®¹ | Googleæœç´¢ |
| æ¨èç³»ç»Ÿ | æ‰¾ç›¸ä¼¼çš„ç‰©å“ | Netflixæ¨è |
| RAG | æ£€ç´¢ç›¸å…³æ–‡æ¡£ | ChatGPTçŸ¥è¯†åº“ |
| èšç±» | è‡ªåŠ¨åˆ†ç»„ | æ–°é—»åˆ†ç±» |
| å¼‚å¸¸æ£€æµ‹ | æ‰¾ç¦»ç¾¤ç‚¹ | æ¬ºè¯ˆæ£€æµ‹ |

**å…±åŒç‚¹ï¼š** éƒ½éœ€è¦"æ¯”è¾ƒç›¸ä¼¼åº¦"

---

### å¡ç‰‡8ï¼šä¸åŒæ¨¡æ€çš„Embedding ğŸŒˆ

**æ–‡æœ¬Embedding**
```python
text_emb = text_model.encode("ä¸€åªå¯çˆ±çš„çŒ«")
```

**å›¾åƒEmbedding**
```python
image_emb = image_model.encode(cat_image)
```

**å¤šæ¨¡æ€Embeddingï¼ˆCLIPï¼‰**
```python
# æ–‡å­—å’Œå›¾ç‰‡åœ¨åŒä¸€ä¸ªç©ºé—´ï¼
text_emb = clip.encode_text("ä¸€åªçŒ«")
image_emb = clip.encode_image(cat_photo)
# å¯ä»¥ç”¨æ–‡å­—æœå›¾ç‰‡ï¼Œæˆ–ç”¨å›¾ç‰‡æœæ–‡å­—
```

---

### å¡ç‰‡9ï¼šEmbeddingçš„è´¨é‡è¯„ä¼° ğŸ“ˆ

**å¦‚ä½•åˆ¤æ–­Embeddingå¥½ä¸å¥½ï¼Ÿ**

```python
# å¥½çš„Embeddingåº”è¯¥æ»¡è¶³ï¼š
# 1. ç›¸ä¼¼çš„å†…å®¹è·ç¦»è¿‘
similarity("è‹¹æœ", "é¦™è•‰") > similarity("è‹¹æœ", "ç”µè„‘")

# 2. èƒ½å®Œæˆä¸‹æ¸¸ä»»åŠ¡
# - æ–‡æœ¬åˆ†ç±»å‡†ç¡®ç‡é«˜
# - æœç´¢å¬å›ç‡é«˜
# - èšç±»ç»“æœåˆç†
```

**å¸¸ç”¨è¯„ä¼°æŒ‡æ ‡ï¼š**
- æ£€ç´¢ä»»åŠ¡ï¼šRecall@K, MRR
- åˆ†ç±»ä»»åŠ¡ï¼šAccuracy, F1
- èšç±»ä»»åŠ¡ï¼šSilhouette Score

---

### å¡ç‰‡10ï¼šEmbeddingåœ¨å‘é‡æ•°æ®åº“ä¸­çš„è§’è‰² ğŸ—„ï¸

**å®Œæ•´æµç¨‹ï¼š**
```
1. æ•°æ®å‡†å¤‡
   åŸå§‹æ–‡æ¡£ â†’ åˆ†å— â†’ ç”ŸæˆEmbedding â†’ å­˜å…¥å‘é‡æ•°æ®åº“

2. æŸ¥è¯¢å¤„ç†
   ç”¨æˆ·é—®é¢˜ â†’ ç”ŸæˆEmbedding â†’ å‘é‡æ•°æ®åº“æœç´¢ â†’ è¿”å›Top-K

3. åº”ç”¨ç”Ÿæˆï¼ˆRAGï¼‰
   Top-Kæ–‡æ¡£ + ç”¨æˆ·é—®é¢˜ â†’ å¤§æ¨¡å‹ â†’ æœ€ç»ˆç­”æ¡ˆ
```

**æ ¸å¿ƒï¼š** Embeddingæ˜¯è¿æ¥"äººç±»è¯­è¨€"å’Œ"å‘é‡æ•°æ®åº“"çš„æ¡¥æ¢

---

## 7. ã€3ä¸ªæ ¸å¿ƒæ¦‚å¿µã€‘

### æ ¸å¿ƒæ¦‚å¿µ1ï¼šå‘é‡ç©ºé—´ï¼ˆVector Spaceï¼‰ ğŸŒŒ

**ä¸€å¥è¯å®šä¹‰ï¼š** Embeddingå°†æ‰€æœ‰å†…å®¹æ˜ å°„åˆ°ä¸€ä¸ªå¤šç»´ç©ºé—´ï¼Œç›¸ä¼¼çš„å†…å®¹é å¾—è¿‘ã€‚

```python
import numpy as np
import matplotlib.pyplot as plt

# å‡è®¾åœ¨2Dç©ºé—´å¯è§†åŒ–ï¼ˆå®é™…æ˜¯é«˜ç»´ï¼‰
words = {
    "è‹¹æœ": [0.8, 0.6],
    "é¦™è•‰": [0.7, 0.5],
    "æ©™å­": [0.75, 0.55],
    "Python": [0.1, 0.9],
    "Java": [0.15, 0.85],
    "JavaScript": [0.2, 0.8]
}

# å¯è§†åŒ–
plt.figure(figsize=(8, 6))
for word, coord in words.items():
    plt.scatter(coord[0], coord[1], s=100)
    plt.annotate(word, coord, fontsize=12)

plt.xlabel("ç»´åº¦1ï¼ˆå¯èƒ½è¡¨ç¤ºï¼šé£Ÿç‰© vs ç¼–ç¨‹ï¼‰")
plt.ylabel("ç»´åº¦2ï¼ˆå¯èƒ½è¡¨ç¤ºï¼šå…¶ä»–ç‰¹å¾ï¼‰")
plt.title("Embeddingå‘é‡ç©ºé—´ç¤ºæ„")
plt.grid(True)
# plt.show()
```

```
å‘é‡ç©ºé—´ç¤ºæ„å›¾ï¼š

        ç»´åº¦2 (ç¼–ç¨‹ç›¸å…³)
           â†‘
     1.0   â”‚    Python â— Java â— JavaScript
           â”‚    
     0.5   â”‚ è‹¹æœ â— é¦™è•‰ â— æ©™å­
           â”‚
     0.0   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â†’ ç»´åº¦1
          0.0                   1.0
                         (æ°´æœç›¸å…³)
```

**åœ¨å‘é‡æ•°æ®åº“ä¸­çš„åº”ç”¨ï¼š**
- å‘é‡æ•°æ®åº“å°±æ˜¯é«˜æ•ˆç®¡ç†è¿™ä¸ª"ç©ºé—´"çš„å·¥å…·
- ç´¢å¼•ç®—æ³•ï¼ˆHNSWã€IVFï¼‰åŠ é€Ÿç©ºé—´ä¸­çš„è¿‘é‚»æœç´¢

---

### æ ¸å¿ƒæ¦‚å¿µ2ï¼šè¯­ä¹‰è·ç¦»ï¼ˆSemantic Distanceï¼‰ ğŸ“

**ä¸€å¥è¯å®šä¹‰ï¼š** ä¸¤ä¸ªembeddingä¹‹é—´çš„è·ç¦»åæ˜ å®ƒä»¬è¯­ä¹‰çš„å·®å¼‚ç¨‹åº¦ã€‚

```python
import numpy as np

def cosine_distance(v1, v2):
    """ä½™å¼¦è·ç¦» = 1 - ä½™å¼¦ç›¸ä¼¼åº¦"""
    cos_sim = np.dot(v1, v2) / (np.linalg.norm(v1) * np.linalg.norm(v2))
    return 1 - cos_sim

def euclidean_distance(v1, v2):
    """æ¬§æ°è·ç¦»"""
    return np.linalg.norm(np.array(v1) - np.array(v2))

# ç¤ºä¾‹
apple = np.array([0.8, 0.6, 0.1])
banana = np.array([0.7, 0.5, 0.2])
python = np.array([0.1, 0.2, 0.9])

print("è¯­ä¹‰è·ç¦»ï¼ˆä½™å¼¦è·ç¦»ï¼‰ï¼š")
print(f"  è‹¹æœ-é¦™è•‰: {cosine_distance(apple, banana):.4f}  # å° = ç›¸ä¼¼")
print(f"  è‹¹æœ-Python: {cosine_distance(apple, python):.4f}  # å¤§ = ä¸åŒ")

print("\nè¯­ä¹‰è·ç¦»ï¼ˆæ¬§æ°è·ç¦»ï¼‰ï¼š")
print(f"  è‹¹æœ-é¦™è•‰: {euclidean_distance(apple, banana):.4f}")
print(f"  è‹¹æœ-Python: {euclidean_distance(apple, python):.4f}")
```

**åœ¨å‘é‡æ•°æ®åº“ä¸­çš„åº”ç”¨ï¼š**
- æœç´¢æ—¶è®¡ç®—æŸ¥è¯¢å‘é‡ä¸æ‰€æœ‰æ–‡æ¡£å‘é‡çš„è·ç¦»
- è¿”å›è·ç¦»æœ€å°ï¼ˆæœ€ç›¸ä¼¼ï¼‰çš„Top-Kç»“æœ

---

### æ ¸å¿ƒæ¦‚å¿µ3ï¼šæ¨¡å‹ä¸€è‡´æ€§ï¼ˆModel Consistencyï¼‰ ğŸ”’

**ä¸€å¥è¯å®šä¹‰ï¼š** åŒä¸€ä¸ªç³»ç»Ÿä¸­ï¼Œå¿…é¡»ä½¿ç”¨åŒä¸€ä¸ªembeddingæ¨¡å‹ï¼Œå¦åˆ™æ¯”è¾ƒæ— æ„ä¹‰ã€‚

```python
# âŒ é”™è¯¯ç¤ºä¾‹ï¼šæ··ç”¨ä¸åŒæ¨¡å‹

# ç´¢å¼•æ—¶ç”¨æ¨¡å‹A
index_embedding = model_a.encode("æœºå™¨å­¦ä¹ å…¥é—¨")
vector_db.insert(index_embedding)

# æŸ¥è¯¢æ—¶ç”¨æ¨¡å‹Bï¼ˆé”™è¯¯ï¼ï¼‰
query_embedding = model_b.encode("å¦‚ä½•å­¦ä¹ ML")
vector_db.search(query_embedding)  # ç»“æœæ— æ„ä¹‰ï¼

# âœ… æ­£ç¡®ç¤ºä¾‹ï¼šç»Ÿä¸€ä½¿ç”¨åŒä¸€æ¨¡å‹

# å®šä¹‰é¡¹ç›®ä½¿ç”¨çš„æ¨¡å‹
EMBEDDING_MODEL = "text-embedding-ada-002"

# ç´¢å¼•æ—¶
index_embedding = get_embedding("æœºå™¨å­¦ä¹ å…¥é—¨", model=EMBEDDING_MODEL)
vector_db.insert(index_embedding)

# æŸ¥è¯¢æ—¶ï¼ˆä½¿ç”¨åŒä¸€æ¨¡å‹ï¼‰
query_embedding = get_embedding("å¦‚ä½•å­¦ä¹ ML", model=EMBEDDING_MODEL)
vector_db.search(query_embedding)  # ç»“æœæœ‰æ„ä¹‰ï¼
```

**åœ¨å‘é‡æ•°æ®åº“ä¸­çš„åº”ç”¨ï¼š**
- å»ºåº“å’ŒæŸ¥è¯¢å¿…é¡»ä½¿ç”¨åŒä¸€æ¨¡å‹
- è¿ç§»æ¨¡å‹æ—¶éœ€è¦é‡æ–°ç”Ÿæˆæ‰€æœ‰embedding
- è¿™æ˜¯ä¸€ä¸ªå¸¸è§çš„"å‘"ï¼

---

## 8. ã€1ä¸ªç±»æ¯”ã€‘ç”¨å‰ç«¯å¼€å‘ç†è§£Embedding

### ç±»æ¯”1ï¼šEmbedding = JSONåºåˆ—åŒ– ğŸ“¦

**å‰ç«¯æ¦‚å¿µï¼š** å°†JavaScriptå¯¹è±¡è½¬æ¢æˆJSONå­—ç¬¦ä¸²ä»¥ä¾¿ä¼ è¾“å’Œå­˜å‚¨

```javascript
// JavaScriptå¯¹è±¡ï¼ˆäººç±»å¯è¯»ï¼‰
const user = {
  name: "å¼ ä¸‰",
  age: 25,
  hobbies: ["ç¼–ç¨‹", "è¯»ä¹¦"]
};

// JSONåºåˆ—åŒ–ï¼ˆè®¡ç®—æœºå¯å¤„ç†ï¼‰
const jsonString = JSON.stringify(user);
// '{"name":"å¼ ä¸‰","age":25,"hobbies":["ç¼–ç¨‹","è¯»ä¹¦"]}'
```

**Embeddingï¼š** å°†æ–‡æœ¬è½¬æ¢æˆå‘é‡ä»¥ä¾¿è®¡ç®—æœºå¤„ç†

```python
# æ–‡æœ¬ï¼ˆäººç±»å¯è¯»ï¼‰
text = "å¼ ä¸‰æ˜¯ä¸€ä¸ª25å²çš„ç¨‹åºå‘˜ï¼Œå–œæ¬¢ç¼–ç¨‹å’Œè¯»ä¹¦"

# Embeddingï¼ˆè®¡ç®—æœºå¯å¤„ç†ï¼‰
embedding = model.encode(text)
# [0.23, -0.15, 0.89, ..., 0.34]
```

**ç›¸åŒç‚¹ï¼š**
- éƒ½æ˜¯"ç¼–ç "è¿‡ç¨‹
- éƒ½æ˜¯ä¸ºäº†è®©è®¡ç®—æœºèƒ½å¤„ç†

**ä¸åŒç‚¹ï¼š**
- JSONæ˜¯æ— æŸçš„ï¼Œå¯ä»¥è¿˜åŸ
- Embeddingæ˜¯æœ‰æŸçš„ï¼Œä¸èƒ½è¿˜åŸåŸæ–‡ï¼Œä½†ä¿ç•™äº†è¯­ä¹‰

---

### ç±»æ¯”2ï¼šEmbedding = CSSå˜é‡æ˜ å°„ ğŸ¨

```css
/* CSSå˜é‡ï¼šæŠŠæŠ½è±¡æ¦‚å¿µæ˜ å°„åˆ°å…·ä½“å€¼ */
:root {
  --primary-color: #3498db;
  --secondary-color: #2ecc71;
  --danger-color: #e74c3c;
}

/* ä½¿ç”¨æ—¶é€šè¿‡å˜é‡åè®¿é—® */
.button {
  background: var(--primary-color);
}
```

**Embeddingç±»ä¼¼ï¼š**
```python
# Embeddingï¼šæŠŠæŠ½è±¡æ–‡æœ¬æ˜ å°„åˆ°å‘é‡ç©ºé—´çš„åæ ‡

# "ä¸»è¦é¢œè‰²" ç±»ä¼¼äº "æœºå™¨å­¦ä¹ " çš„æ¦‚å¿µ
machine_learning = [0.8, 0.7, 0.6, ...]  # ç©ºé—´ä¸­çš„ä¸€ä¸ªåæ ‡

# "æ¬¡è¦é¢œè‰²" ç±»ä¼¼äº "æ·±åº¦å­¦ä¹ " çš„æ¦‚å¿µ  
deep_learning = [0.75, 0.72, 0.58, ...]  # é™„è¿‘çš„å¦ä¸€ä¸ªåæ ‡

# ç›¸è¿‘çš„æ¦‚å¿µï¼Œåœ¨ç©ºé—´ä¸­è·ç¦»ä¹Ÿç›¸è¿‘
```

---

### ç±»æ¯”3ï¼šå‘é‡ç©ºé—´ = DOMåæ ‡ç³» ğŸ“

```javascript
// DOMå…ƒç´ çš„ä½ç½®ï¼ˆ2Dåæ ‡ï¼‰
const element = document.querySelector('.box');
const rect = element.getBoundingClientRect();

console.log(`ä½ç½®: (${rect.x}, ${rect.y})`);
// ä½ç½®: (100, 200)

// åˆ¤æ–­ä¸¤ä¸ªå…ƒç´ æ˜¯å¦"é è¿‘"
function isNear(elem1, elem2, threshold = 50) {
  const rect1 = elem1.getBoundingClientRect();
  const rect2 = elem2.getBoundingClientRect();
  
  const distance = Math.sqrt(
    Math.pow(rect1.x - rect2.x, 2) + 
    Math.pow(rect1.y - rect2.y, 2)
  );
  
  return distance < threshold;
}
```

**Embeddingç±»ä¼¼ï¼š**
```python
# æ–‡æœ¬çš„ä½ç½®ï¼ˆé«˜ç»´åæ ‡ï¼‰
text1_position = model.encode("æœºå™¨å­¦ä¹ ")  # [0.8, 0.7, ...]
text2_position = model.encode("æ·±åº¦å­¦ä¹ ")  # [0.75, 0.72, ...]

# åˆ¤æ–­ä¸¤ä¸ªæ–‡æœ¬æ˜¯å¦"è¯­ä¹‰ç›¸è¿‘"
def is_similar(text1_emb, text2_emb, threshold=0.8):
    similarity = cosine_similarity(text1_emb, text2_emb)
    return similarity > threshold
```

**ç›¸åŒç‚¹ï¼š**
- éƒ½æ˜¯ç”¨åæ ‡è¡¨ç¤ºä½ç½®
- éƒ½å¯ä»¥è®¡ç®—"è·ç¦»"æ¥åˆ¤æ–­è¿œè¿‘

**ä¸åŒç‚¹ï¼š**
- DOMæ˜¯2Dåæ ‡
- Embeddingæ˜¯é«˜ç»´åæ ‡ï¼ˆ768ç»´ã€1536ç»´ï¼‰

---

### ç±»æ¯”4ï¼šEmbeddingæœç´¢ = æ¨¡ç³Šè·¯ç”±åŒ¹é… ğŸ›£ï¸

```javascript
// ä¼ ç»Ÿè·¯ç”±ï¼šç²¾ç¡®åŒ¹é…
const routes = [
  { path: '/home', component: Home },
  { path: '/about', component: About },
  { path: '/contact', component: Contact }
];

function exactMatch(url) {
  return routes.find(r => r.path === url);
}

exactMatch('/home');    // âœ… æ‰¾åˆ°
exactMatch('/hom');     // âŒ æ‰¾ä¸åˆ°ï¼ˆå·®ä¸€ä¸ªå­—æ¯ï¼‰
```

```javascript
// Embeddingæœç´¢ï¼šæ¨¡ç³ŠåŒ¹é…ï¼ˆè¯­ä¹‰ç›¸ä¼¼å³å¯ï¼‰
const documents = [
  { text: 'å¦‚ä½•å­¦ä¹ ç¼–ç¨‹', embedding: [0.8, 0.7, ...] },
  { text: 'Pythonå…¥é—¨æ•™ç¨‹', embedding: [0.75, 0.68, ...] },
  { text: 'ä»Šå¤©å¤©æ°”ä¸é”™', embedding: [0.1, 0.2, ...] }
];

function semanticSearch(query) {
  const queryEmb = getEmbedding(query);
  
  // æ‰¾æœ€ç›¸ä¼¼çš„ï¼ˆä¸éœ€è¦å®Œå…¨åŒ¹é…ï¼‰
  return documents
    .map(doc => ({
      ...doc,
      similarity: cosineSimilarity(queryEmb, doc.embedding)
    }))
    .sort((a, b) => b.similarity - a.similarity)
    .slice(0, 3);
}

semanticSearch('æ€ä¹ˆå¼€å§‹å­¦Python');  
// âœ… æ‰¾åˆ° "å¦‚ä½•å­¦ä¹ ç¼–ç¨‹" å’Œ "Pythonå…¥é—¨æ•™ç¨‹"
// å³ä½¿æŸ¥è¯¢è¯ä¸å®Œå…¨åŒ¹é…ï¼Œä¹Ÿèƒ½æ‰¾åˆ°è¯­ä¹‰ç›¸å…³çš„å†…å®¹
```

---

### ç±»æ¯”5ï¼šæ¨¡å‹ä¸€è‡´æ€§ = APIç‰ˆæœ¬ä¸€è‡´æ€§ ğŸ”„

```javascript
// å‰ç«¯APIè°ƒç”¨ï¼šç‰ˆæœ¬å¿…é¡»ä¸€è‡´

// âŒ é”™è¯¯ï¼šåç«¯v1è¿”å›çš„æ•°æ®ï¼Œç”¨v2çš„è§£æå™¨å¤„ç†
const dataFromV1 = fetch('/api/v1/users');
parseV2Response(dataFromV1);  // æ ¼å¼ä¸åŒ¹é…ï¼

// âœ… æ­£ç¡®ï¼šç‰ˆæœ¬ä¸€è‡´
const dataFromV1 = fetch('/api/v1/users');
parseV1Response(dataFromV1);  // OK
```

**Embeddingç±»ä¼¼ï¼š**
```python
# âŒ é”™è¯¯ï¼šç”¨æ¨¡å‹Aç”Ÿæˆçš„embeddingï¼Œå’Œæ¨¡å‹Bçš„embeddingæ¯”è¾ƒ
doc_emb = model_a.encode("æ–‡æ¡£å†…å®¹")
query_emb = model_b.encode("æŸ¥è¯¢å†…å®¹")
similarity(doc_emb, query_emb)  # æ— æ„ä¹‰ï¼

# âœ… æ­£ç¡®ï¼šåŒä¸€æ¨¡å‹
doc_emb = model_a.encode("æ–‡æ¡£å†…å®¹")
query_emb = model_a.encode("æŸ¥è¯¢å†…å®¹")
similarity(doc_emb, query_emb)  # OK
```

---

### ç±»æ¯”æ€»ç»“ ğŸ¯

| Embeddingæ¦‚å¿µ | å‰ç«¯ç±»æ¯” | å…³é”®ç‚¹ |
|--------------|---------|-------|
| Embeddingè¿‡ç¨‹ | JSONåºåˆ—åŒ– | ç¼–ç è½¬æ¢ |
| å‘é‡ç©ºé—´ | DOMåæ ‡ç³» | ä½ç½®è¡¨ç¤º |
| è¯­ä¹‰è·ç¦» | å…ƒç´ è·ç¦»è®¡ç®— | è¿œè¿‘åˆ¤æ–­ |
| è¯­ä¹‰æœç´¢ | æ¨¡ç³Šè·¯ç”±åŒ¹é… | ä¸è¦æ±‚ç²¾ç¡® |
| æ¨¡å‹ä¸€è‡´æ€§ | APIç‰ˆæœ¬ä¸€è‡´ | å¿…é¡»ç»Ÿä¸€ |

---

## 9. ã€ç¬¬ä¸€æ€§åŸç†ã€‘Embeddingçš„æœ¬è´¨

### ä»€ä¹ˆæ˜¯ç¬¬ä¸€æ€§åŸç†ï¼Ÿ

**ç¬¬ä¸€æ€§åŸç†**ï¼šå›åˆ°äº‹ç‰©æœ€åŸºæœ¬çš„çœŸç†ï¼Œä»æºå¤´æ€è€ƒé—®é¢˜

### Embeddingçš„ç¬¬ä¸€æ€§åŸç† ğŸ¯

#### 1. æœ€åŸºç¡€çš„é—®é¢˜

**é—®é¢˜ï¼šè®¡ç®—æœºåªèƒ½å¤„ç†æ•°å­—ï¼Œå¦‚ä½•è®©å®ƒ"ç†è§£"äººç±»çš„è¯­è¨€å’Œå›¾åƒï¼Ÿ**

```
äººç±»ä¸–ç•Œ          è®¡ç®—æœºä¸–ç•Œ
 æ–‡å­—              ???
 å›¾åƒ              ???
 å£°éŸ³              ???
```

#### 2. æœ€åŸºç¡€çš„è§£å†³æ–¹æ¡ˆ

**ç­”æ¡ˆï¼šå»ºç«‹æ˜ å°„å…³ç³»ï¼**

```
äººç±»ä¸–ç•Œ          æ˜ å°„          è®¡ç®—æœºä¸–ç•Œ
 æ–‡å­—       â†’    Embedding    â†’    å‘é‡ï¼ˆæ•°å­—ï¼‰
 å›¾åƒ       â†’    Embedding    â†’    å‘é‡ï¼ˆæ•°å­—ï¼‰
 å£°éŸ³       â†’    Embedding    â†’    å‘é‡ï¼ˆæ•°å­—ï¼‰
```

**Embedding = ä»äººç±»ä¸–ç•Œåˆ°æ•°å­—ä¸–ç•Œçš„"ç¿»è¯‘å™¨"**

#### 3. ä¸ºä»€ä¹ˆæ˜ å°„æˆå‘é‡ï¼Ÿ

**ä»ç¬¬ä¸€æ€§åŸç†æ¨å¯¼ï¼š**

```
1. è®¡ç®—æœºéœ€è¦å¤„ç†è¯­ä¹‰ 
   â†“
2. è¯­ä¹‰éœ€è¦å¯æ¯”è¾ƒï¼ˆåˆ¤æ–­ç›¸ä¼¼/ä¸åŒï¼‰
   â†“
3. æ¯”è¾ƒéœ€è¦åº¦é‡ï¼ˆè·ç¦»/ç›¸ä¼¼åº¦ï¼‰
   â†“
4. åº¦é‡éœ€è¦æ•°å­¦å¯¹è±¡
   â†“
5. å‘é‡æ˜¯æœ€é€‚åˆçš„æ•°å­¦å¯¹è±¡ï¼š
   - å¯ä»¥è®¡ç®—è·ç¦»ï¼ˆæ¬§æ°ã€ä½™å¼¦ï¼‰
   - å¯ä»¥è¿ç®—ï¼ˆåŠ å‡ï¼‰
   - å¯ä»¥å­˜å‚¨å’Œç´¢å¼•
   â†“
6. æ‰€ä»¥ï¼šè¯­ä¹‰ â†’ å‘é‡
```

#### 4. ä¸ºä»€ä¹ˆç›¸ä¼¼çš„å†…å®¹å‘é‡æ¥è¿‘ï¼Ÿ

**è¿™æ˜¯Embeddingæ¨¡å‹çš„è®­ç»ƒç›®æ ‡ï¼**

```python
# è®­ç»ƒæ—¶çš„ä¼˜åŒ–ç›®æ ‡ï¼ˆå¯¹æ¯”å­¦ä¹ ï¼‰
loss = 0
for (positive_pair, negative_pair) in training_data:
    # ç›¸ä¼¼å†…å®¹åº”è¯¥é è¿‘
    loss += distance(embed(text1), embed(text2_similar))
    
    # ä¸åŒå†…å®¹åº”è¯¥è¿œç¦»
    loss -= distance(embed(text1), embed(text3_different))

# ä¼˜åŒ–æ¨¡å‹ä½¿lossæœ€å°
# â†’ è‡ªç„¶è€Œç„¶ï¼Œç›¸ä¼¼çš„å†…å®¹å‘é‡å°±ä¼šæ¥è¿‘
```

#### 5. Embeddingçš„ä¸‰å±‚ä»·å€¼ï¼ˆä»ç¬¬ä¸€æ€§åŸç†æ¨å¯¼ï¼‰

##### ä»·å€¼1ï¼šè¡¨ç¤ºï¼ˆè®©æœºå™¨"çœ‹è§"ï¼‰

```
æ²¡æœ‰Embeddingï¼š
  è®¡ç®—æœºçœ‹åˆ° "è‹¹æœ" è¿™ä¸¤ä¸ªå­— â†’ åªæ˜¯ç¼–ç  [232, 149, 186, ...]
  ä¸çŸ¥é“è¿™æ˜¯æ°´æœï¼Œä¸çŸ¥é“å¯ä»¥åƒï¼Œä¸çŸ¥é“å’Œé¦™è•‰ç±»ä¼¼

æœ‰äº†Embeddingï¼š
  è®¡ç®—æœºçœ‹åˆ° "è‹¹æœ" â†’ [0.8, 0.6, 0.1, ...]
  ç¬¬1ç»´é«˜ â†’ é£Ÿç‰©ç›¸å…³
  ç¬¬2ç»´é«˜ â†’ æ°´æœç±»åˆ«
  â†’ è®¡ç®—æœº"ç†è§£"äº†è¿™æ˜¯ä¸€ä¸ªæ°´æœ
```

##### ä»·å€¼2ï¼šè®¡ç®—ï¼ˆè®©æœºå™¨"æ€è€ƒ"ï¼‰

```python
# ç»å…¸æ¡ˆä¾‹ï¼šking - man + woman â‰ˆ queen

king = embed("å›½ç‹")      # [0.8, 0.9, 0.3, ...]
man = embed("ç”·äºº")       # [0.2, 0.8, 0.1, ...]
woman = embed("å¥³äºº")     # [0.2, 0.1, 0.9, ...]

result = king - man + woman
# = [0.8-0.2+0.2, 0.9-0.8+0.1, 0.3-0.1+0.9]
# = [0.8, 0.2, 1.1]
# â‰ˆ queençš„å‘é‡ [0.8, 0.2, 1.0]

# è®¡ç®—æœºé€šè¿‡å‘é‡è¿ç®—ï¼Œ"æ¨ç†"å‡ºäº†æ¦‚å¿µå…³ç³»ï¼
```

##### ä»·å€¼3ï¼šæ¯”è¾ƒï¼ˆè®©æœºå™¨"åˆ¤æ–­"ï¼‰

```python
query = "å¦‚ä½•å­¦ä¹ Python"
doc1 = "Pythonå…¥é—¨æ•™ç¨‹"
doc2 = "ä»Šå¤©å¤©æ°”ä¸é”™"

# è®¡ç®—ç›¸ä¼¼åº¦
sim1 = similarity(embed(query), embed(doc1))  # é«˜
sim2 = similarity(embed(query), embed(doc2))  # ä½

# è®¡ç®—æœºèƒ½"åˆ¤æ–­"å“ªä¸ªæ–‡æ¡£æ›´ç›¸å…³ï¼
```

#### 6. ä»ç¬¬ä¸€æ€§åŸç†æ¨å¯¼å‘é‡æ•°æ®åº“

```
1. æœ‰äº†Embeddingï¼Œç°å®å¯¹è±¡å˜æˆäº†å‘é‡
   â†“
2. ä¸šåŠ¡éœ€æ±‚ï¼šå¿«é€Ÿæ‰¾åˆ°ç›¸ä¼¼çš„å‘é‡
   â†“
3. æš´åŠ›æœç´¢å¤ªæ…¢ï¼ˆO(n)ï¼Œnå¯èƒ½ä¸Šäº¿ï¼‰
   â†“
4. éœ€è¦ä¸“é—¨çš„ç´¢å¼•ç»“æ„ï¼ˆHNSWã€IVFï¼‰
   â†“
5. éœ€è¦ä¸“é—¨çš„å­˜å‚¨ç³»ç»Ÿ
   â†“
6. è¯ç”Ÿäº†å‘é‡æ•°æ®åº“ï¼

å‘é‡æ•°æ®åº“ = Embeddingçš„é«˜æ•ˆå­˜å‚¨å’Œæ£€ç´¢ç³»ç»Ÿ
```

#### 7. ä¸€å¥è¯æ€»ç»“ç¬¬ä¸€æ€§åŸç†

**Embeddingæ˜¯å°†äººç±»ä¸–ç•Œæ˜ å°„åˆ°æ•°å­¦ç©ºé—´çš„ç¿»è¯‘å™¨ï¼Œä½¿è®¡ç®—æœºèƒ½å¤Ÿè¡¨ç¤ºã€è®¡ç®—å’Œæ¯”è¾ƒè¯­ä¹‰ï¼Œæ˜¯AIç†è§£ä¸–ç•Œçš„åŸºç¡€ã€‚**

---

## 10. ã€ä¸€å¥è¯æ€»ç»“ã€‘

**Embeddingæ˜¯å°†æ–‡æœ¬ã€å›¾åƒç­‰éç»“æ„åŒ–æ•°æ®è½¬æ¢æˆå›ºå®šç»´åº¦å‘é‡çš„æŠ€æœ¯ï¼Œå®ƒè®©ç›¸ä¼¼çš„å†…å®¹åœ¨å‘é‡ç©ºé—´ä¸­è·ç¦»æ›´è¿‘ï¼Œä½¿è®¡ç®—æœºèƒ½å¤Ÿç†è§£è¯­ä¹‰å¹¶è¿›è¡Œç›¸ä¼¼åº¦æœç´¢ï¼Œæ˜¯å‘é‡æ•°æ®åº“å’ŒRAGç³»ç»Ÿçš„æ ¸å¿ƒåŸºç¡€ã€‚**

---

## é™„å½•ï¼šå¿«é€Ÿå‚è€ƒå¡ ğŸ“‹

### æ ¸å¿ƒæ¦‚å¿µé€ŸæŸ¥

```python
# 1. ä½¿ç”¨OpenAIç”ŸæˆEmbedding
from openai import OpenAI
client = OpenAI()
response = client.embeddings.create(
    input="ä½ çš„æ–‡æœ¬",
    model="text-embedding-ada-002"
)
embedding = response.data[0].embedding

# 2. ä½¿ç”¨å¼€æºæ¨¡å‹ï¼ˆå…è´¹ï¼‰
from sentence_transformers import SentenceTransformer
model = SentenceTransformer('all-MiniLM-L6-v2')
embedding = model.encode("ä½ çš„æ–‡æœ¬")

# 3. è®¡ç®—ä½™å¼¦ç›¸ä¼¼åº¦
import numpy as np
def cosine_similarity(v1, v2):
    return np.dot(v1, v2) / (np.linalg.norm(v1) * np.linalg.norm(v2))

# 4. æ‰¹é‡ç”Ÿæˆ
texts = ["æ–‡æœ¬1", "æ–‡æœ¬2", "æ–‡æœ¬3"]
embeddings = model.encode(texts)
```

### å¸¸ç”¨æ¨¡å‹é€ŸæŸ¥

| æ¨¡å‹ | ç»´åº¦ | ç‰¹ç‚¹ | é€‚ç”¨åœºæ™¯ |
|------|------|------|---------|
| text-embedding-ada-002 | 1536 | é€šç”¨æ€§å¼º | å•†ç”¨é¡¹ç›® |
| all-MiniLM-L6-v2 | 384 | é€Ÿåº¦å¿« | åŸå‹å¼€å‘ |
| all-mpnet-base-v2 | 768 | å¹³è¡¡ | é€šç”¨åœºæ™¯ |
| text-embedding-3-large | 3072 | æœ€æ–°æœ€å¼º | é«˜ç²¾åº¦éœ€æ±‚ |

### å­¦ä¹ æ£€æŸ¥æ¸…å• âœ…

- [ ] èƒ½ç”¨ä¸€å¥è¯è§£é‡ŠEmbeddingæ˜¯ä»€ä¹ˆ
- [ ] ç†è§£Embeddingå’ŒOne-Hotçš„åŒºåˆ«
- [ ] çŸ¥é“ä¸åŒæ¨¡å‹çš„Embeddingä¸èƒ½ç›´æ¥æ¯”è¾ƒ
- [ ] ä¼šä½¿ç”¨APIæˆ–å¼€æºæ¨¡å‹ç”ŸæˆEmbedding
- [ ] èƒ½è®¡ç®—ä¸¤ä¸ªEmbeddingçš„ç›¸ä¼¼åº¦
- [ ] ç†è§£å‘é‡ç©ºé—´çš„æ¦‚å¿µ
- [ ] ç†è§£Embeddingåœ¨RAGä¸­çš„ä½œç”¨
- [ ] çŸ¥é“Embeddingçš„ç¬¬ä¸€æ€§åŸç†

### ä¸‹ä¸€æ­¥å­¦ä¹  ğŸš€

æŒæ¡äº†Embeddingçš„åŸºæœ¬æ¦‚å¿µåï¼Œå»ºè®®å­¦ä¹ ï¼š

1. **è¯­ä¹‰ç›¸ä¼¼æ€§åŸç†**ï¼šä¸ºä»€ä¹ˆç›¸ä¼¼å†…å®¹çš„å‘é‡æ¥è¿‘
2. **å¸¸è§Embeddingæ¨¡å‹**ï¼šå„ç§æ¨¡å‹çš„ç‰¹ç‚¹å’Œé€‰æ‹©
3. **ç»´åº¦ä¸è´¨é‡å…³ç³»**ï¼šå¦‚ä½•é€‰æ‹©åˆé€‚çš„ç»´åº¦

**å­¦ä¹ è·¯å¾„ï¼š**
```
ä»€ä¹ˆæ˜¯Embeddingï¼ˆå½“å‰ï¼‰
    â†“
è¯­ä¹‰ç›¸ä¼¼æ€§åŸç†
    â†“
å¸¸è§Embeddingæ¨¡å‹
    â†“
ç»´åº¦ä¸è´¨é‡å…³ç³»
    â†“
å‘é‡æ•°æ®åº“å®æˆ˜
```

---

## å‚è€ƒèµ„æº ğŸ“š

1. **OpenAI Embeddingæ–‡æ¡£**ï¼šhttps://platform.openai.com/docs/guides/embeddings
2. **Sentence Transformers**ï¼šhttps://www.sbert.net/
3. **Word2VecåŸç†**ï¼šhttps://arxiv.org/abs/1301.3781
4. **å‘é‡æ•°æ®åº“é€‰å‹**ï¼šPineconeã€Qdrantã€Milvusã€Weaviate

---

**ç»“è¯­ï¼š** Embeddingæ˜¯ç†è§£å‘é‡æ•°æ®åº“çš„æ ¸å¿ƒæ¦‚å¿µï¼ŒæŒæ¡äº†å®ƒï¼Œä½ å°±æ‹¥æœ‰äº†è®©AIç†è§£è¯­ä¹‰çš„"é­”æ³•"ï¼ğŸ¯
